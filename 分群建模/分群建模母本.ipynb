{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Memory as mm\n",
    "import time\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.tree as tree\n",
    "from sklearn.tree import _tree\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwei/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pydotplus\n",
    "from sklearn.externals.six import StringIO \n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "from sklearn.externals.six import StringIO \n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 80.06 MB\n",
      "Memory usage after optimization is: 18.90 MB\n",
      "Decreased by 76.4%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 169.02 MB\n",
      "Memory usage after optimization is: 60.05 MB\n",
      "Decreased by 64.5%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 391.41 MB\n",
      "Memory usage after optimization is: 186.81 MB\n",
      "Decreased by 52.3%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 44.48 MB\n",
      "Memory usage after optimization is: 13.34 MB\n",
      "Decreased by 70.0%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 329.14 MB\n",
      "Memory usage after optimization is: 128.99 MB\n",
      "Decreased by 60.8%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 1076.36 MB\n",
      "Memory usage after optimization is: 369.17 MB\n",
      "Decreased by 65.7%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 649.38 MB\n",
      "Memory usage after optimization is: 251.30 MB\n",
      "Decreased by 61.3%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 649.38 MB\n",
      "Memory usage after optimization is: 224.61 MB\n",
      "Decreased by 65.4%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# card sheets\n",
    "\n",
    "card_ol = pd.read_csv('Card_processed/card_ol.csv',encoding='GBK').drop('ol_neg',axis=1)\n",
    "card_ol = mm.reduce_mem_usage(card_ol)\n",
    "\n",
    "card_gene = pd.read_csv('Card_processed/card_gene.csv',encoding='GBK')\n",
    "card_gene = mm.reduce_mem_usage(card_gene)\n",
    "zero_sum = (card_gene.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/card_gene.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "card_gene = card_gene[['id']+kept]\n",
    "\n",
    "card_volatility = pd.read_csv('Card_processed/card_volatility.csv',encoding='GBK')\n",
    "card_volatility = mm.reduce_mem_usage(card_volatility)\n",
    "zero_sum = (card_volatility.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/card_volatility.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "card_volatility = card_volatility[['id']+kept]\n",
    "\n",
    "card_window = pd.read_csv('Card_processed/card_window.csv',encoding='GBK')\n",
    "card_window = mm.reduce_mem_usage(card_window)\n",
    "\n",
    "card_cross1 = pd.read_csv('Card_processed/card_cross1.csv',encoding='GBK')\n",
    "card_cross1 = mm.reduce_mem_usage(card_cross1)\n",
    "zero_sum = (card_cross1.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/card_cross1.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "card_cross1 = card_cross1[['id']+kept]\n",
    "\n",
    "card_cross2 = pd.read_csv('Card_processed/card_cross2.csv',encoding='GBK')\n",
    "card_cross2 = mm.reduce_mem_usage(card_cross2)\n",
    "zero_sum = (card_cross2.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/card_cross2.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "card_cross2 = card_cross2[['id']+kept]\n",
    "\n",
    "card_cross3 = pd.read_csv('Card_processed/card_cross3.csv',encoding='GBK')\n",
    "card_cross3 = mm.reduce_mem_usage(card_cross3)\n",
    "zero_sum = (card_cross3.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/card_cross3.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "card_cross3 = card_cross3[['id']+kept]\n",
    "\n",
    "card_cross4 = pd.read_csv('Card_processed/card_cross4.csv',encoding='GBK')\n",
    "card_cross4 = mm.reduce_mem_usage(card_cross4)\n",
    "zero_sum = (card_cross4.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/card_cross4.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "card_cross4 = card_cross4[['id']+kept]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "card = pd.merge(card_ol,card_gene,on='id',how='inner')\n",
    "card = pd.merge(card,card_window,on='id',how='inner')\n",
    "card = pd.merge(card,card_volatility,on='id',how='inner')\n",
    "card = pd.merge(card,card_cross1,on='id',how='inner')\n",
    "card = pd.merge(card,card_cross2,on='id',how='inner')\n",
    "card = pd.merge(card,card_cross3,on='id',how='inner')\n",
    "card = pd.merge(card,card_cross4,on='id',how='inner')\n",
    "del card_ol,card_window,card_volatility,card_gene,card_cross1,card_cross2,card_cross3,card_cross4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 59.74 MB\n",
      "Memory usage after optimization is: 16.60 MB\n",
      "Decreased by 72.2%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 132.76 MB\n",
      "Memory usage after optimization is: 47.30 MB\n",
      "Decreased by 64.4%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 238.97 MB\n",
      "Memory usage after optimization is: 112.85 MB\n",
      "Decreased by 52.8%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 33.19 MB\n",
      "Memory usage after optimization is: 9.96 MB\n",
      "Decreased by 70.0%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 723.54 MB\n",
      "Memory usage after optimization is: 275.48 MB\n",
      "Decreased by 61.9%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 544.31 MB\n",
      "Memory usage after optimization is: 220.71 MB\n",
      "Decreased by 59.5%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 245.60 MB\n",
      "Memory usage after optimization is: 87.95 MB\n",
      "Decreased by 64.2%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 484.57 MB\n",
      "Memory usage after optimization is: 210.76 MB\n",
      "Decreased by 56.5%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 305.35 MB\n",
      "Memory usage after optimization is: 109.53 MB\n",
      "Decreased by 64.1%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 424.83 MB\n",
      "Memory usage after optimization is: 179.22 MB\n",
      "Decreased by 57.8%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# loan sheets\n",
    "\n",
    "loan_ol = pd.read_csv('Loan_processed/loan_ol.csv',encoding='GBK').drop('ol_neg',axis=1)\n",
    "loan_ol = mm.reduce_mem_usage(loan_ol)\n",
    "\n",
    "loan_gene = pd.read_csv('Loan_processed/loan_gene.csv',encoding='GBK')\n",
    "loan_gene = mm.reduce_mem_usage(loan_gene)\n",
    "zero_sum = (loan_gene.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_gene.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_gene = loan_gene[['id']+kept]\n",
    "\n",
    "loan_volatility = pd.read_csv('Loan_processed/loan_volatility.csv',encoding='GBK')\n",
    "loan_volatility = mm.reduce_mem_usage(loan_volatility)\n",
    "zero_sum = (loan_volatility.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_volatility.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_volatility = loan_volatility[['id']+kept]\n",
    "\n",
    "loan_window = pd.read_csv('Loan_processed/loan_window.csv',encoding='GBK')\n",
    "loan_window = mm.reduce_mem_usage(loan_window)\n",
    "\n",
    "loan_cross1 = pd.read_csv('Loan_processed/loan_cross1.csv',encoding='GBK')\n",
    "loan_cross1 = mm.reduce_mem_usage(loan_cross1)\n",
    "zero_sum = (loan_cross1.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_cross1.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_cross1 = loan_cross1[['id']+kept]\n",
    "\n",
    "loan_cross2 = pd.read_csv('Loan_processed/loan_cross2.csv',encoding='GBK')\n",
    "loan_cross2 = mm.reduce_mem_usage(loan_cross2)\n",
    "zero_sum = (loan_cross2.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_cross2.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_cross2 = loan_cross2[['id']+kept]\n",
    "\n",
    "loan_cross3 = pd.read_csv('Loan_processed/loan_cross3.csv',encoding='GBK')\n",
    "loan_cross3 = mm.reduce_mem_usage(loan_cross3)\n",
    "zero_sum = (loan_cross3.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_cross3.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_cross3 = loan_cross3[['id']+kept]\n",
    "\n",
    "loan_cross4 = pd.read_csv('Loan_processed/loan_cross4.csv',encoding='GBK')\n",
    "loan_cross4 = mm.reduce_mem_usage(loan_cross4)\n",
    "zero_sum = (loan_cross4.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_cross4.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_cross4 = loan_cross4[['id']+kept]\n",
    "\n",
    "loan_cross5 = pd.read_csv('Loan_processed/loan_cross5.csv',encoding='GBK')\n",
    "loan_cross5 = mm.reduce_mem_usage(loan_cross5)\n",
    "zero_sum = (loan_cross5.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_cross5.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_cross5 = loan_cross5[['id']+kept]\n",
    "\n",
    "loan_cross6 = pd.read_csv('Loan_processed/loan_cross6.csv',encoding='GBK')\n",
    "loan_cross6 = mm.reduce_mem_usage(loan_cross6)\n",
    "zero_sum = (loan_cross6.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_cross6.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_cross6 = loan_cross6[['id']+kept]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan = pd.merge(loan_ol,loan_gene,on='id',how='inner')\n",
    "loan = pd.merge(loan,loan_window,on='id',how='inner')\n",
    "loan = pd.merge(loan,loan_volatility,on='id',how='inner')\n",
    "loan = pd.merge(loan,loan_cross1,on='id',how='inner')\n",
    "loan = pd.merge(loan,loan_cross2,on='id',how='inner')\n",
    "loan = pd.merge(loan,loan_cross3,on='id',how='inner')\n",
    "loan = pd.merge(loan,loan_cross4,on='id',how='inner')\n",
    "loan = pd.merge(loan,loan_cross5,on='id',how='inner')\n",
    "loan = pd.merge(loan,loan_cross6,on='id',how='inner')\n",
    "del loan_ol,loan_window,loan_volatility,loan_gene,loan_cross1,loan_cross2,loan_cross3,loan_cross4,loan_cross5,loan_cross6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((870050, 59), (1165961, 69))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan.shape,card.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 1460.67 MB\n",
      "Memory usage after optimization is: 226.04 MB\n",
      "Decreased by 84.5%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 461.43 MB\n",
      "Memory usage after optimization is: 104.44 MB\n",
      "Decreased by 77.4%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 174.59 MB\n",
      "Memory usage after optimization is: 48.33 MB\n",
      "Decreased by 72.3%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 24.94 MB\n",
      "Memory usage after optimization is: 24.94 MB\n",
      "Decreased by 0.0%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Jiben Sheet\n",
    "\n",
    "jiben_dys = pd.read_csv('Jiben_processed/jiben_processed_dys0429.csv')\n",
    "jiben_dys = mm.reduce_mem_usage(jiben_dys)\n",
    "\n",
    "jiben_hy = pd.read_csv('Jiben_processed/jiben_processed_hy0429.csv')\n",
    "jiben_hy = mm.reduce_mem_usage(jiben_hy)\n",
    "\n",
    "jiben_yjy = pd.read_csv('Jiben_processed/yjy_jiben.csv')\n",
    "jiben_yjy = mm.reduce_mem_usage(jiben_yjy)\n",
    "\n",
    "observe_date = pd.read_csv('refer/jiben_vars_new_20200424.csv',skiprows=list(range(779441,779509)),\n",
    "                           encoding=\"ISO-8859-1\",error_bad_lines=False,low_memory=False)[['id','nasrdw_recd_date']]\n",
    "observe_date = mm.reduce_mem_usage(observe_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "jiben = pd.merge(jiben_dys,jiben_hy,how='inner',on='id')\n",
    "jiben = pd.merge(jiben,jiben_yjy,how='inner',on='id')\n",
    "jiben = pd.merge(observe_date,jiben,how='inner',on='id')\n",
    "\n",
    "jiben.drop(['age18~21','age21~24','age24~28','age28~49','age50'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del jiben_dys,jiben_hy,jiben_yjy,observe_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_sum = (jiben.iloc[:,2:]==0).sum()\n",
    "# cond = (zero_sum/jiben.shape[0])<=0.7\n",
    "# kept = list(zero_sum.loc[cond].index.values)\n",
    "# jiben = jiben[['id','bad']+kept]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add New Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 57.85 MB\n",
      "Memory usage after optimization is: 31.82 MB\n",
      "Decreased by 45.0%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 57.85 MB\n",
      "Memory usage after optimization is: 31.82 MB\n",
      "Decreased by 45.0%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 69.42 MB\n",
      "Memory usage after optimization is: 39.05 MB\n",
      "Decreased by 43.7%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 110.67 MB\n",
      "Memory usage after optimization is: 23.34 MB\n",
      "Decreased by 78.9%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 138.98 MB\n",
      "Memory usage after optimization is: 31.27 MB\n",
      "Decreased by 77.5%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "loan_3_sum = pd.read_csv('Loan_processed/Record3_sumamounts.csv')\n",
    "loan_3_sum = mm.reduce_mem_usage(loan_3_sum)\n",
    "zero_sum = (loan_3_sum.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_3_sum.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_3_sum = loan_3_sum[['id']+kept]\n",
    "\n",
    "loan_6_sum = pd.read_csv('Loan_processed/Record6_sumamounts.csv')\n",
    "loan_6_sum = mm.reduce_mem_usage(loan_6_sum)\n",
    "zero_sum = (loan_6_sum.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_6_sum.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_6_sum = loan_6_sum[['id']+kept]\n",
    "\n",
    "loan_latest = pd.read_csv('Loan_processed/loan_latest.csv')\n",
    "loan_latest = mm.reduce_mem_usage(loan_latest)\n",
    "zero_sum = (loan_latest.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_latest.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_latest = loan_latest[['id']+kept]\n",
    "\n",
    "loan_latest24 = pd.read_csv('Loan_processed/loan_24_derive.csv')\n",
    "loan_latest24 = mm.reduce_mem_usage(loan_latest24)\n",
    "zero_sum = (loan_latest24.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/loan_latest24.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "loan_latest24 = loan_latest24[['id']+kept]\n",
    "\n",
    "card_latest24 = pd.read_csv('Card_processed/card_24_derive.csv')\n",
    "card_latest24 = mm.reduce_mem_usage(card_latest24)\n",
    "zero_sum = (card_latest24.iloc[:,1:]==0).sum()\n",
    "cond = (zero_sum/card_latest24.shape[0])<=0.6\n",
    "kept = list(zero_sum.loc[cond].index.values)\n",
    "card_latest24 = card_latest24[['id']+kept]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Big Sheet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 1664.88 MB\n",
      "Memory usage after optimization is: 1190.98 MB\n",
      "Decreased by 28.5%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "big_sheet = pd.merge(jiben,card,on='id',how='left')\n",
    "big_sheet = pd.merge(big_sheet,loan,on='id',how='left')\n",
    "big_sheet = pd.merge(big_sheet,loan_3_sum,on='id',how='left')\n",
    "big_sheet = pd.merge(big_sheet,loan_6_sum,on='id',how='left')\n",
    "big_sheet = pd.merge(big_sheet,loan_latest,on='id',how='left')\n",
    "big_sheet = pd.merge(big_sheet,loan_latest24,on='id',how='left')\n",
    "big_sheet = pd.merge(big_sheet,card_latest24,on='id',how='left')\n",
    "big_sheet = mm.reduce_mem_usage(big_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_sheet.drop('opendate2',axis=1,inplace=True)\n",
    "big_sheet.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loan_3_sum,loan_6_sum,loan_latest,loan_latest24,card_latest24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_sheet = big_sheet.replace([np.inf,-np.inf],np.nan)\n",
    "big_sheet.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1634600, 309)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_sheet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nasrdw_recd_date</th>\n",
       "      <th>bad</th>\n",
       "      <th>age_na</th>\n",
       "      <th>age_ol</th>\n",
       "      <th>var_jb_1_18~21</th>\n",
       "      <th>var_jb_1_21~24</th>\n",
       "      <th>var_jb_1_24~28</th>\n",
       "      <th>var_jb_1_28~49</th>\n",
       "      <th>var_jb_1_50</th>\n",
       "      <th>...</th>\n",
       "      <th>scheduledpaymentamount</th>\n",
       "      <th>actualpaymentamount</th>\n",
       "      <th>loan_24_*</th>\n",
       "      <th>loan_24_/</th>\n",
       "      <th>loan_24_N</th>\n",
       "      <th>loan_24_nan</th>\n",
       "      <th>card_24_*</th>\n",
       "      <th>card_24_/</th>\n",
       "      <th>card_24_nan</th>\n",
       "      <th>card_24_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56a13c06ee2f2a0db827d0d9450213d3</td>\n",
       "      <td>20171220</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c6342448d324d0e99e0d89175a2da335</td>\n",
       "      <td>20171229</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82e5383d3b794f2555dadb7e37e9108e</td>\n",
       "      <td>20180208</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a5e91382fc770d3bf1c3845d113d85a5</td>\n",
       "      <td>20180306</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57e97bcbc4090d25d1c783fcb071b6df</td>\n",
       "      <td>20180314</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 310 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id nasrdw_recd_date bad  age_na  age_ol  \\\n",
       "0  56a13c06ee2f2a0db827d0d9450213d3         20171220   G       0       0   \n",
       "1  c6342448d324d0e99e0d89175a2da335         20171229   G       0       0   \n",
       "2  82e5383d3b794f2555dadb7e37e9108e         20180208   G       0       0   \n",
       "3  a5e91382fc770d3bf1c3845d113d85a5         20180306   G       0       0   \n",
       "4  57e97bcbc4090d25d1c783fcb071b6df         20180314   G       0       0   \n",
       "\n",
       "   var_jb_1_18~21  var_jb_1_21~24  var_jb_1_24~28  var_jb_1_28~49  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               1   \n",
       "2               0               0               0               1   \n",
       "3               0               0               0               1   \n",
       "4               0               0               0               1   \n",
       "\n",
       "   var_jb_1_50  ...  scheduledpaymentamount  actualpaymentamount  loan_24_*  \\\n",
       "0            1  ...                     0.0                  0.0        0.0   \n",
       "1            0  ...                     0.0                  0.0        0.0   \n",
       "2            0  ...                     0.0                  0.0        0.0   \n",
       "3            0  ...                     0.0                  0.0        0.0   \n",
       "4            0  ...                     0.0                  0.0        0.0   \n",
       "\n",
       "   loan_24_/  loan_24_N  loan_24_nan  card_24_*  card_24_/  card_24_nan  \\\n",
       "0        0.0        0.0          1.0       38.0        0.0          5.0   \n",
       "1        0.0        0.0          0.0        0.0        0.0          0.0   \n",
       "2        0.0        0.0          0.0        0.0        0.0          0.0   \n",
       "3        0.0        0.0          0.0        0.0        0.0          0.0   \n",
       "4        0.0        0.0          0.0        0.0        0.0          0.0   \n",
       "\n",
       "   card_24_N  \n",
       "0       34.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "\n",
       "[5 rows x 310 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_sheet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del jiben,card,loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = big_sheet['bad']=='N'\n",
    "cond2 = big_sheet['bad']=='R'\n",
    "cond3 = big_sheet['bad']=='G'\n",
    "cond4 = big_sheet['bad']=='B'\n",
    "\n",
    "cond_1 = cond3|cond4\n",
    "cond_2 = cond1\n",
    "\n",
    "big_sheet_N = big_sheet.loc[cond_2].reset_index(drop=True)\n",
    "big_sheet_GB = big_sheet.loc[cond_1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond2017 = big_sheet_GB['nasrdw_recd_date']<'20180101'\n",
    "cond201801 = ((big_sheet_GB['nasrdw_recd_date']>'20171231')&(big_sheet_GB['nasrdw_recd_date']<'20180201'))\n",
    "cond201802 = ((big_sheet_GB['nasrdw_recd_date']>'20180131')&(big_sheet_GB['nasrdw_recd_date']<'20180301'))\n",
    "cond201803 = ((big_sheet_GB['nasrdw_recd_date']>'20180228')&(big_sheet_GB['nasrdw_recd_date']<'20180401'))\n",
    "cond201804 = ((big_sheet_GB['nasrdw_recd_date']>'20180331')&(big_sheet_GB['nasrdw_recd_date']<'20180501'))\n",
    "cond201805 = ((big_sheet_GB['nasrdw_recd_date']>'20180430')&(big_sheet_GB['nasrdw_recd_date']<'20180601'))\n",
    "cond201806 = ((big_sheet_GB['nasrdw_recd_date']>'20180531')&(big_sheet_GB['nasrdw_recd_date']<'20180701'))\n",
    "cond201807 = ((big_sheet_GB['nasrdw_recd_date']>'20180630')&(big_sheet_GB['nasrdw_recd_date']<'20180801'))\n",
    "cond201808 = ((big_sheet_GB['nasrdw_recd_date']>'20180731')&(big_sheet_GB['nasrdw_recd_date']<'20180901'))\n",
    "cond201809 = ((big_sheet_GB['nasrdw_recd_date']>'20180831')&(big_sheet_GB['nasrdw_recd_date']<'20181001'))\n",
    "cond201810 = ((big_sheet_GB['nasrdw_recd_date']>'20180930')&(big_sheet_GB['nasrdw_recd_date']<'20181101'))\n",
    "cond201811 = ((big_sheet_GB['nasrdw_recd_date']>'20181031')&(big_sheet_GB['nasrdw_recd_date']<'20181201'))\n",
    "cond201812 = ((big_sheet_GB['nasrdw_recd_date']>'20181130')&(big_sheet_GB['nasrdw_recd_date']<'20190101'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwei/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "big_sheet_GB['month_mark'] = cond2017*2017\n",
    "big_sheet_GB['month_mark'].loc[cond201801] = 1\n",
    "big_sheet_GB['month_mark'].loc[cond201802] = 2\n",
    "big_sheet_GB['month_mark'].loc[cond201803] = 3\n",
    "big_sheet_GB['month_mark'].loc[cond201804] = 4\n",
    "big_sheet_GB['month_mark'].loc[cond201805] = 5\n",
    "big_sheet_GB['month_mark'].loc[cond201806] = 6\n",
    "big_sheet_GB['month_mark'].loc[cond201807] = 7\n",
    "big_sheet_GB['month_mark'].loc[cond201808] = 8\n",
    "big_sheet_GB['month_mark'].loc[cond201809] = 9\n",
    "big_sheet_GB['month_mark'].loc[cond201810] = 10\n",
    "big_sheet_GB['month_mark'].loc[cond201811] = 11\n",
    "big_sheet_GB['month_mark'].loc[cond201812] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2017,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n",
       "         12,    1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_sheet_GB['month_mark'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_sheet_GB.drop('nasrdw_recd_date',axis=1,inplace=True)\n",
    "big_sheet_N.drop('nasrdw_recd_date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G    888245\n",
       "B     96956\n",
       "Name: bad, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_sheet_GB['bad'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450609, 308)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_sheet_N.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outside Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_data = pd.read_csv('outside_data/outside_data_0430.csv',encoding='GBK')\n",
    "outside_data = mm.reduce_mem_usage(outside_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_data.drop(['公积金数据','养老金数据','资产表数据'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 树分群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeSplit():\n",
    "    \n",
    "    def __init__(self,df,depths,min_leaf):\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.depths = depths\n",
    "        self.min_leaf = min_leaf\n",
    "\n",
    "    def splitGB_NR(self):\n",
    "        \n",
    "        cond1 = self.df['bad']=='N'\n",
    "        cond2 = self.df['bad']=='R'\n",
    "        cond3 = self.df['bad']=='G'\n",
    "        cond4 = self.df['bad']=='B'\n",
    "\n",
    "        cond_1 = cond3|cond4\n",
    "        cond_2 = cond1\n",
    "\n",
    "        self.df_NR = self.df.loc[cond_2].reset_index(drop=True)\n",
    "        self.df_GB = self.df.loc[cond_1].reset_index(drop=True)\n",
    "        \n",
    "        self.df_GB['bad'].replace({'R':'B'},inplace=True)\n",
    "\n",
    "        return self.df_GB,self.df_NR\n",
    "    \n",
    "    def XY_split(self):\n",
    "        \n",
    "        self.X_GB = self.df_GB.drop(['id','bad'],axis=1)\n",
    "        self.y_GB = self.df_GB['bad']\n",
    "        \n",
    "        self.X_NR = self.df_NR.drop(['id','bad'],axis=1)\n",
    "        \n",
    "    def FitTree(self):\n",
    "        \n",
    "        print('start to split by tree')\n",
    "        start = time.time()\n",
    "        tree_clf = DecisionTreeClassifier(random_state=2,max_depth=self.depths,\n",
    "                                          min_samples_leaf=self.min_leaf)\n",
    "        \n",
    "        tree_clf.fit(self.X_GB,self.y_GB)\n",
    "        self.tree_clf = tree_clf\n",
    "        \n",
    "        self.SubsetID_NR = tree_clf.apply(self.X_NR)\n",
    "        \n",
    "        self.grandson_split_ID_NR = np.unique(self.SubsetID_NR)\n",
    "        end = time.time()\n",
    "        print('tree split finished, time cost {}seconds'.format(np.round(end-start,2)))\n",
    "        \n",
    "    def Visualize(self):\n",
    "        \n",
    "        self.dot_data = StringIO()\n",
    "        tree.export_graphviz(self.tree_clf, \n",
    "                             out_file=self.dot_data,\n",
    "                             feature_names=self.X_GB.columns,\n",
    "                             filled=True, \n",
    "                             rounded=True, \n",
    "                             special_characters=True)\n",
    "\n",
    "        self.graph = pydotplus.graph_from_dot_data(self.dot_data.getvalue()) \n",
    "        \n",
    "        \n",
    "    def SubSetsRecord(self):\n",
    "        \n",
    "        print('start to get leaves\\' info')\n",
    "        start = time.time()\n",
    "        dec_path = self.tree_clf.decision_path(self.X_GB)\n",
    "        self.all_nodes = collections.defaultdict(list)\n",
    "\n",
    "        for d, dec in enumerate(dec_path):\n",
    "            for i in range(self.tree_clf.tree_.node_count):\n",
    "                if dec.toarray()[0][i] == 1:\n",
    "                    self.all_nodes[i].append(d) \n",
    "        \n",
    "        self.NR_nodes = {}\n",
    "        for i in self.grandson_split_ID_NR:\n",
    "            self.NR_nodes[i] = np.where(self.SubsetID_NR==i)\n",
    "        \n",
    "        self.end_nodes = {}\n",
    "        for i in self.grandson_split_ID_NR:\n",
    "            self.end_nodes[i] = self.all_nodes[i]\n",
    "                \n",
    "        end = time.time()\n",
    "        print('leaves\\' info restored, time cost {}seconds'.format(np.round(end-start,2)))\n",
    "            \n",
    "    def ExtractRules(self):\n",
    "        \n",
    "        tree_ = self.tree_clf.tree_\n",
    "        self.feature_name = [self.feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "                             for i in tree_.feature]\n",
    "\n",
    "        #print(\"def tree({}):\".format(\", \".join(feature_names)))\n",
    "\n",
    "        def recurse(node, depth):\n",
    "            indent = \"  \" * depth\n",
    "            if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "                self.split_name = self.feature_name[node]\n",
    "                self.threshold = tree_.threshold[node]\n",
    "\n",
    "#                 print('name',name)\n",
    "#                 print('threshold',threshold)\n",
    "\n",
    "                print(\"{}if {} <= {}:\".format(indent, self.split_name, self.threshold))\n",
    "                recurse(tree_.children_left[node], depth + 1)\n",
    "                print(\"{}else:  # if {} > {}\".format(indent, self.split_name, self.threshold))\n",
    "                recurse(tree_.children_right[node], depth + 1)\n",
    "            else:\n",
    "                print(\"{}return {}\".format(indent, tree_.value[node]))\n",
    "\n",
    "        recurse(0, 1)\n",
    "    \n",
    "    \n",
    "    def Ignite(self):\n",
    "        \n",
    "        self.splitGB_NR()\n",
    "        self.XY_split()\n",
    "        self.FitTree()\n",
    "        self.Visualize()\n",
    "        self.SubSetsRecord()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**执行树分群**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_data_tree_dict = {}\n",
    "for i in [3,4,5]:\n",
    "    outside_data_tree = TreeSplit(outside_data,i,1000)\n",
    "    outside_data_tree.Ignite()\n",
    "    outside_data_tree_dict[i] = outside_data_tree\n",
    "outside_data_tree3 = outside_data_tree_dict[3]\n",
    "outside_data_tree4 = outside_data_tree_dict[4]\n",
    "outside_data_tree5 = outside_data_tree_dict[5]\n",
    "del outside_data_tree_dict,outside_data_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_data_tree3 = TreeSplit(outside_data,3,100000)\n",
    "outside_data_tree3.Ignite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(outside_data_tree3.graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(outside_data_tree4.graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(outside_data_tree5.graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf0 = LogisticRegression()\n",
    "lr_clf1 = LogisticRegression(max_iter=1000)\n",
    "lr_clf2 = LogisticRegression(max_iter=1000,C=0.1)\n",
    "\n",
    "\n",
    "def LR(lr_clf,X_GB,y_GB,X_N_df,i,return_dict,if_month=1):\n",
    "    \n",
    "    all_fts = set(X_GB.columns)\n",
    "    ft_stat = (X_GB != 0).sum() \n",
    "    cond = (ft_stat==0)\n",
    "    not_used = set(ft_stat.loc[cond].index.values)\n",
    "    used_fts = list(all_fts-not_used)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    if if_month==1:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_GB.iloc[:,:-1] = scaler.fit_transform(X_GB.iloc[:,:-1])\n",
    "        cond = X_GB['month_mark'].isin(month_pairs[i]['train'])\n",
    "        X_train = X_GB.loc[cond].drop('month_mark',axis=1).reset_index(drop=True)\n",
    "        y_train = y_GB.loc[cond]\n",
    "        \n",
    "        cond = X_GB['month_mark'].isin(month_pairs[i]['test'])\n",
    "        X_test = X_GB.loc[cond].drop('month_mark',axis=1).reset_index(drop=True)\n",
    "        y_test = y_GB.loc[cond]        \n",
    "        \n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_GB_scaled = scaler.fit_transform(X_GB)\n",
    "        X_train,X_test,y_train,y_test = train_test_split(X_GB_scaled,y_GB,test_size=0.3,random_state=2)\n",
    "    \n",
    "    scaler = MinMaxScaler()    \n",
    "    X_N_scaled = scaler.fit_transform(X_N_df.drop(['id','bad'],axis=1))\n",
    "    \n",
    "    train_good = sum(y_train)\n",
    "    train_bad = len(y_train)-train_good\n",
    "\n",
    "    test_good = sum(y_test)\n",
    "    test_bad = len(y_test)-test_good\n",
    "    \n",
    "    total_good = train_good + test_good  \n",
    "    total_bad = train_bad + test_bad\n",
    "        \n",
    "    print('start to train and estimate model{}'.format(i))\n",
    "    print()\n",
    "    lr_clf.fit(X_train,y_train)\n",
    "    \n",
    "    pred = lr_clf.predict_proba(X_test)\n",
    "    pred_train = lr_clf.predict_proba(X_train)\n",
    "    \n",
    "    auc = roc_auc_score(y_test,pred[:,1])\n",
    "    auc_train = roc_auc_score(y_train,pred_train[:,1])\n",
    "    \n",
    "    fpr,tpr,thresholds = roc_curve(y_test,pred[:,1])\n",
    "    ks = max(tpr-fpr)\n",
    "    \n",
    "    fpr,tpr,thresholds = roc_curve(y_train,pred_train[:,1])\n",
    "    ks_train = max(tpr-fpr)\n",
    "    \n",
    "    NR_pred = lr_clf.predict_proba(X_N_scaled)[:,1]\n",
    "    NR_pred_df = pd.DataFrame({'id':X_N_df['id'],'pred_good_prob':NR_pred})\n",
    "    NR_pred_df['split'] = i\n",
    "    \n",
    "    coefs = lr_clf.coef_[0]\n",
    "#     del lr_clf\n",
    "    \n",
    "    return_dict[i] = [auc,ks,y_test,pred[:,1],auc_train,ks_train,y_train,pred_train[:,1],used_fts,\n",
    "                      train_good,train_bad,test_good,test_bad,total_good,total_bad,coefs,NR_pred_df,lr_clf]\n",
    "    end = time.time()\n",
    "    print('model {} finished, time spent {}seconds'.format(i,np.round(end-start,2)))\n",
    "    \n",
    "\n",
    "    \n",
    "    return [auc,ks,y_test,pred[:,1],auc_train,ks_train,y_train,pred_train[:,1],\n",
    "            used_fts,train_good,train_bad,test_good,test_bad,total_good,total_bad,\n",
    "            coefs,NR_pred_df,lr_clf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_clf0 = lgb.LGBMClassifier()\n",
    "\n",
    "lgbm_clf0_1 = lgb.LGBMClassifier(n_estimators=40,max_depth=5,lambda_l1=0.5,lambda_l2=5)\n",
    "\n",
    "lgbm_clf1 = lgb.LGBMClassifier(max_bin=150,n_estimators=500,min_sum_hessian_in_leaf=1,\n",
    "                             learning_rate=0.01,bagging_fraction = 0.8,objective='binary',\n",
    "                             colsample_bytree = 0.7,feature_fraction = 0.8,\n",
    "                             lambda_l1 = 0.5,lambda_l2 = 4.0,max_depth =5,importance_type='gain',\n",
    "                             subsample = 0.7,silent=True,min_child_samples = 50)  \n",
    "\n",
    "lgbm_clf2 = lgb.LGBMClassifier(max_bin=130,n_estimators=500,min_sum_hessian_in_leaf=1,\n",
    "                             learning_rate=0.01,bagging_fraction = 0.7,objective='binary',\n",
    "                             colsample_bytree = 0.7,feature_fraction = 0.7,\n",
    "                             lambda_l1 = 1.2,lambda_l2 = 7.0,max_depth =5,importance_type='gain',\n",
    "                             subsample = 0.75,silent=True,min_child_samples = 60) \n",
    "\n",
    "lgbm_clf3 = lgb.LGBMClassifier(max_bin=110,n_estimators=300,min_sum_hessian_in_leaf=1,\n",
    "                             learning_rate=0.01,bagging_fraction = 0.7,objective='binary',\n",
    "                             colsample_bytree = 0.7,feature_fraction = 0.7,\n",
    "                             lambda_l1 = 4.0,lambda_l2 = 9.0,max_depth =5,importance_type='gain',\n",
    "                             subsample = 0.65,silent=True,min_child_samples = 60) \n",
    "\n",
    "lgbm_clf4 = lgb.LGBMClassifier(max_bin=80,n_estimators=250,min_sum_hessian_in_leaf=1,\n",
    "                             learning_rate=0.01,bagging_fraction = 0.7,objective='binary',\n",
    "                             colsample_bytree = 0.6,feature_fraction = 0.7,\n",
    "                             lambda_l1 = 6.0,lambda_l2 = 11.0,max_depth =4,importance_type='gain',\n",
    "                             subsample = 0.65,silent=True,min_child_samples = 60) \n",
    "\n",
    "lgbm_clf5 = lgb.LGBMClassifier(max_bin=80,n_estimators=180,min_sum_hessian_in_leaf=1,\n",
    "                             learning_rate=0.01,bagging_fraction = 0.7,objective='binary',\n",
    "                             colsample_bytree = 0.6,feature_fraction = 0.7,\n",
    "                             lambda_l1 = 6.0,lambda_l2 = 12.0,max_depth =4,importance_type='gain',\n",
    "                             subsample = 0.6,silent=True,min_child_samples = 60) \n",
    "\n",
    "lgbm_clf6 = lgb.LGBMClassifier(max_bin=40,n_estimators=100,min_sum_hessian_in_leaf=1,\n",
    "                             learning_rate=0.01,bagging_fraction = 0.7,objective='binary',\n",
    "                             colsample_bytree = 0.5,feature_fraction = 0.6,\n",
    "                             lambda_l1 = 8.0,lambda_l2 = 13.0,max_depth =4,importance_type='gain',\n",
    "                             subsample = 0.5,silent=True,min_child_samples = 70) \n",
    "\n",
    "def LGBM(lgbm_clf,X_GB,y_GB,X_N_df,i,if_month=1):\n",
    "    \n",
    "    start = time.time()\n",
    "    print('start to train LGBM model{}'.format(i))\n",
    "    sk = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    if if_month==1:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_GB.iloc[:,:-1] = scaler.fit_transform(X_GB.iloc[:,:-1])\n",
    "        cond = X_GB['month_mark'].isin(month_pairs[i]['train'])\n",
    "        X_train = X_GB.loc[cond].drop('month_mark',axis=1).values\n",
    "        Y_train = y_GB.loc[cond]\n",
    "        \n",
    "        cond = X_GB['month_mark'].isin(month_pairs[i]['test'])\n",
    "        X_test = X_GB.loc[cond].drop('month_mark',axis=1).values\n",
    "        Y_test = y_GB.loc[cond] \n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        X_GB_scaled = scaler.fit_transform(X_GB)\n",
    "        X_train,X_test,Y_train,Y_test = train_test_split(X_GB_scaled,y_GB,test_size=0.3,random_state=2)\n",
    "\n",
    "    scaler = MinMaxScaler()    \n",
    "    X_N_scaled = scaler.fit_transform(X_N_df.drop(['id','bad'],axis=1))\n",
    "    \n",
    "    train_good = sum(Y_train)\n",
    "    train_bad = len(Y_train)-train_good\n",
    "\n",
    "    test_good = sum(Y_test)\n",
    "    test_bad = len(Y_test)-test_good\n",
    "    \n",
    "    total_good = train_good + test_good  \n",
    "    total_bad = train_bad + test_bad  \n",
    "        \n",
    "    preds = []\n",
    "    preds_train = []   \n",
    "    \n",
    "    NR_preds = []\n",
    "    \n",
    "    ftimps = []\n",
    "    \n",
    "    for train_index, val_index in sk.split(X_train, Y_train):\n",
    "\n",
    "        x_train, x_val = X_train[train_index,:], X_train[val_index,:]\n",
    "        y_train, y_val = Y_train.iloc[train_index], Y_train.iloc[val_index]\n",
    "\n",
    "        lgbm_clf.fit(x_train, y_train, eval_set=[(x_val, y_val)],early_stopping_rounds=20)\n",
    "\n",
    "        preds.append(lgbm_clf.predict_proba(X_test)[:,1])\n",
    "        preds_train.append(lgbm_clf.predict_proba(X_train)[:,1])\n",
    "        \n",
    "        NR_preds.append(lgbm_clf.predict_proba(X_N_scaled)[:,1])\n",
    "        \n",
    "        ftimps.append(lgbm_clf.feature_importances_)\n",
    "\n",
    "    pred = np.mean(preds,axis=0)\n",
    "    pred_train = np.mean(preds_train,axis=0)\n",
    "        \n",
    "    NR_pred = np.mean(NR_preds,axis=0)\n",
    "    \n",
    "    ftimp = np.mean(ftimps,axis=0)\n",
    "    \n",
    "    auc = roc_auc_score(Y_test,pred)\n",
    "    auc_train = roc_auc_score(Y_train,pred_train)\n",
    "    \n",
    "    fpr,tpr,thresholds= roc_curve(Y_test,pred)\n",
    "    ks = max(tpr-fpr)\n",
    "    GB_thres = 1 - thresholds[np.argmax(tpr-fpr)]\n",
    "    \n",
    "    fpr,tpr,thresholds= roc_curve(Y_train,pred_train)\n",
    "    ks_train = max(tpr-fpr)\n",
    " \n",
    "    NR_pred_df = pd.DataFrame({'id':X_N_df['id'],'pred_good_prob':NR_pred})\n",
    "    NR_pred_df['split'] = i\n",
    "    \n",
    "    end = time.time()\n",
    "    print('LGBM model{} finished, time spent {}seconds'.format(i,np.round(end-start,2)))\n",
    "    print()\n",
    "    \n",
    "#     del lgbm_clf\n",
    "    \n",
    "    return [auc,ks,Y_test,pred,auc_train,ks_train,\n",
    "            Y_train,pred_train,train_good,train_bad,\n",
    "            test_good,test_bad,total_good,total_bad,\n",
    "            ftimp,NR_pred_df,GB_thres,lgbm_clf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf0 = xgb.XGBClassifier()\n",
    "\n",
    "xgb_clf1 = xgb.XGBClassifier(colsample_bytree=0.7, gamma=0,importance_type='gain',\n",
    "           eta=0.01, max_depth=6,min_child_weight=1, \n",
    "           n_estimators=500,objective='binary:logistic', random_state=2, \n",
    "           reg_alpha=0.3,reg_lambda=1,subsample=0.7)\n",
    "\n",
    "xgb_clf2 = xgb.XGBClassifier(colsample_bytree=0.7, gamma=1,importance_type='gain',\n",
    "           eta=0.01, max_depth=5,min_child_weight=1, \n",
    "           n_estimators=400,objective='binary:logistic', random_state=2, \n",
    "           reg_alpha=2,reg_lambda=1,subsample=0.6)\n",
    "\n",
    "def XGB(X_GB,y_GB,X_N_df,i,xgb_clf):\n",
    "    \n",
    "    start = time.time()\n",
    "    print('start to train XGB model{}'.format(i))\n",
    "    scaler = MinMaxScaler()\n",
    "    X_GB_scaled = scaler.fit_transform(X_GB)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X_GB_scaled,y_GB,test_size=0.3,random_state=2)\n",
    "    \n",
    "    scaler = MinMaxScaler()    \n",
    "    X_N_scaled = scaler.fit_transform(X_N_df.drop(['id','nasrdw_recd_date','bad'],axis=1))\n",
    "    \n",
    "    train_good = sum(y_train)\n",
    "    train_bad = len(y_train)-train_good\n",
    "\n",
    "    test_good = sum(y_test)\n",
    "    test_bad = len(y_test)-test_good\n",
    "    \n",
    "    total_good = train_good + test_good  \n",
    "    total_bad = train_bad + test_bad\n",
    "    \n",
    "    X_dmatrix = xgb.DMatrix(X_train,label=y_train)\n",
    "#     xgb_clf = xgb.XGBClassifier(**params)\n",
    "    xgb_clf.fit(X_train,y_train)\n",
    "    \n",
    "#     eval_set = [(X_test, y_test)]\n",
    "#     xgb_clf.fit(X_train, y_train, eval_metric=\"auc\", eval_set=eval_set, verbose=False)\n",
    "    \n",
    "    pred = xgb_clf.predict_proba(X_test)\n",
    "    pred_train = xgb_clf.predict_proba(X_train)\n",
    "    \n",
    "    auc = roc_auc_score(y_test,pred[:,1])\n",
    "    auc_train = roc_auc_score(y_train,pred_train[:,1])\n",
    "    \n",
    "    fpr,tpr,thresholds= roc_curve(y_test,pred[:,1])\n",
    "    ks = max(tpr-fpr)\n",
    "    fpr,tpr,thresholds= roc_curve(y_train,pred_train[:,1])\n",
    "    ks_train = max(tpr-fpr)   \n",
    "    \n",
    "    NR_pred = xgb_clf.predict_proba(X_N_scaled)[:,1]\n",
    "    NR_pred_df = pd.DataFrame({'id':X_N_df['id'],'pred_good_prob':NR_pred})\n",
    "    NR_pred_df['split'] = i\n",
    "    \n",
    "#     return_dict[i] = xgb_clf,auc,ks,y_test,pred[:,1]\n",
    "    end = time.time()\n",
    "    print('model {} finished, time spent {}seconds'.format(i,np.round(end-start,2)))\n",
    "    \n",
    "    return xgb_clf,auc,ks,y_test,pred[:,1],auc_train,ks_train,y_train,pred_train[:,1],train_good,train_bad,test_good,test_bad,total_good,total_bad,xgb_clf.feature_importances_,NR_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class Discrete(object):\n",
    "    def __init__(self, x, y, continue_list,dis_num):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.continue_list = continue_list\n",
    "        self.dis_num = dis_num\n",
    "\n",
    "# 判断特征数据是否为离散型\n",
    "    def feature_discretion(self):\n",
    "        X = self.x\n",
    "        temp = np.array([])\n",
    "        for i in range(0, X.shape[-1]):\n",
    "            x1 = X.iloc[:, i]\n",
    "            if x1.name in self.continue_list:\n",
    "                x2 = self.Quantile_Discretiton(x1)\n",
    "                temp = np.r_[temp, x2]\n",
    "            else:\n",
    "                temp = np.r_[temp, x1]\n",
    "        return temp.reshape(X.T.shape).T\n",
    "\n",
    "# 分位数分箱进行离散化\n",
    "    def Quantile_Discretiton(self,X):\n",
    "        res = np.array([0] * X.shape[-1], dtype=int)\n",
    "        X = np.array([X])\n",
    "        for i in range(self.dis_num):\n",
    "            times = 100 // self.dis_num\n",
    "            point1 = np.percentile(X, i * times)\n",
    "            point2 = np.percentile(X, (i + 1) * times)\n",
    "            index_s = np.where((X >= point1) & (X <= point2))[1]\n",
    "            x1 = X.T[index_s]\n",
    "            mask = np.in1d(X, x1)\n",
    "            res[mask] = (i + 1)\n",
    "        return res\n",
    "\n",
    "\n",
    "class Feature_filter(Discrete):\n",
    "    def __init__(self, X, Y, event, con_list,dis_num):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.event = event\n",
    "        self.con_list = con_list\n",
    "        self.dis_num = dis_num\n",
    "\n",
    "# 结果筛选\n",
    "    def Result(self, percent=0.5):\n",
    "        X1 = Discrete(self.X,self.Y,self.con_list,self.dis_num).feature_discretion()\n",
    "        iv = self.Woe_iv(X1)\n",
    "        iv = iv.sort_values(ascending=False)\n",
    "        method_per = int(float(percent * iv.shape[0])) + 1\n",
    "        if method_per >= iv.shape[0]:\n",
    "            res = iv\n",
    "        else:\n",
    "            res = iv[0:method_per]\n",
    "        return res\n",
    "\n",
    "# 求woe值和iv值\n",
    "    def Woe_iv(self, XX):\n",
    "        res_woe = np.array([])\n",
    "        res_iv = np.array([])\n",
    "        for i in range(0, XX.shape[-1]):\n",
    "            X2 = XX[:, i]\n",
    "            woe, iv = self.Woe_single_x(a=X2)\n",
    "            res_woe = np.append(res_woe, woe)\n",
    "            res_iv = np.append(res_iv, iv)\n",
    "        res = pd.Series(res_iv, index=self.X.columns)\n",
    "        return res\n",
    "\n",
    "# 求单个特征的woe值\n",
    "    def Woe_single_x(self, a):\n",
    "        event_total, non_event_total = self.Count_binary(self.Y, self.event)\n",
    "        x_labels = np.unique(a)\n",
    "        woe_dict = {}\n",
    "        iv = 0\n",
    "        for x1 in x_labels:\n",
    "            y1 = self.Y.iloc[np.where(a == x1)[0]]\n",
    "            event_count, non_event_count = self.Count_binary(y1, self.event)\n",
    "            rate_event = 1.0 * event_count / event_total\n",
    "            rate_non_event = 1.0 * non_event_count / non_event_total\n",
    "        # woe无穷大时处理\n",
    "            if rate_event == 0:\n",
    "                print()\n",
    "            elif rate_non_event == 0:\n",
    "                print()\n",
    "            else:\n",
    "                woe1 = math.log(rate_event / rate_non_event)\n",
    "                woe_dict[x1] = woe1\n",
    "                iv += (rate_event - rate_non_event) * woe1\n",
    "        return woe_dict, iv\n",
    "\n",
    "# 计算个数\n",
    "    def Count_binary(self, b, event):\n",
    "        event_count = (b == self.event).sum()\n",
    "        non_event_count = b.shape[-1] - event_count\n",
    "        return event_count, non_event_count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_IV(x,y_encode,IV_cols,return_dict,i):\n",
    "    start = time.time()\n",
    "    print('start to run iv on split{}'.format(i))\n",
    "    result = Feature_filter(x,y_encode,1,IV_cols,5).Result(percent=1)\n",
    "    return_dict[i]=result\n",
    "    end=time.time()\n",
    "    print('time cost {}seconds'.format(np.round(end-start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(outside_data_tree3.graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**统一筛选IV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print('start to run total iv')\n",
    "total_iv = Feature_filter(big_sheet_GB.drop(['id','bad'],axis=1),\n",
    "                          big_sheet_GB['bad'].replace({'G':1,'B':0}),\n",
    "                          1,\n",
    "                          list(big_sheet_GB.drop(['id','bad'],axis=1).columns),\n",
    "                          5).Result(percent=1)\n",
    "mother_iv_fts = list(total_iv.loc[total_iv>0].index.values)\n",
    "end=time.time()\n",
    "print('time cost {}seconds'.format(np.round(end-start,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LR训练模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model mother\n",
    "mother_X = big_sheet_GB.drop(['id','bad'],axis=1)\n",
    "mother_Y = big_sheet_GB['bad'].replace({'G':1,'B':0})\n",
    "[mother_auc,mother_ks,mother_y_test,mother_pred,\n",
    " mother_auc_train,mother_ks_train,mother_y_train,mother_pred_train,\n",
    " mother_used_fts,mother_train_good,mother_train_bad,mother_test_good,\n",
    " mother_test_bad,mother_total_good,mother_total_bad,\n",
    " mother_coefs,mother_lr_NR_pred,mother_lr_clf] = LR(lr_clf0,mother_X,mother_Y,big_sheet_N,'mother',{})\n",
    "mother_Y.value_counts(),mother_auc,mother_ks,mother_auc_train,mother_ks_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresholds= roc_curve(mother_y_test,mother_pred)\n",
    "thres = 1 - thresholds[np.argmax(tpr-fpr)]\n",
    "thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(mother_lr_NR_pred['pred_good_prob']<(1-thres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf0_0 = LogisticRegression()\n",
    "X_GB = big_sheet_GB.drop(['id','bad'],axis=1)\n",
    "Y_GB = big_sheet_GB['bad'].replace({'G':1,'B':0})\n",
    "c = LR(lr_clf0_0,X_GB,Y_GB,big_sheet_N,'pair1',{},1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly recurrent\n",
    "month_pairs = {'pair1':{'train':[1,2,3,4,5,6],'test':[10,11,12]},\n",
    "               'pair2':{'train':[3,4,5,6,7,8],'test':[12,1,2]},\n",
    "               'pair3':{'train':[5,6,7,8,9,10],'test':[2,3,4]},\n",
    "               'pair4':{'train':[7,8,9,10,11,12],'test':[4,5,6]},\n",
    "               'pair5':{'train':[9,10,11,12,1,2],'test':[6,7,8]},\n",
    "               'pair6':{'train':[11,12,1,2,3,4],'test':[8,9,10]}}\n",
    "# month_pair_keys = list(month_pairs.keys())\n",
    "# if __name__ == '__main__':\n",
    "#     manager = mp.Manager()\n",
    "#     lr_month_recurrent_dict = manager.dict()\n",
    "#     jobs = []\n",
    "#     for i in month_pair_keys:\n",
    "#         lr_clf0_0 = LogisticRegression()\n",
    "#         X_GB = big_sheet_GB.drop(['id','bad'],axis=1)\n",
    "#         Y_GB = big_sheet_GB['bad'].replace({'G':1,'B':0})\n",
    "#         X_N = big_sheet_N\n",
    "        \n",
    "#         p = mp.Process(target=LR, args=(lr_clf0_0,X_GB,Y_GB,X_N,i,lr_month_recurrent_dict,1))\n",
    "        \n",
    "#         jobs.append(p)\n",
    "#         p.start()\n",
    "\n",
    "#     for proc in jobs:\n",
    "#         proc.join()\n",
    "        \n",
    "# end = time.time()\n",
    "# print('total time cost of duplicates processing takes {}seconds'.format(np.round(end-start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "end_nodes = outside_data_tree3.end_nodes\n",
    "NR_nodes = outside_data_tree3.NR_nodes\n",
    "\n",
    "list1 = [11,10,7,14,13]\n",
    "list2 = [6,4,3]\n",
    "\n",
    "nodes_id = end_nodes.keys()\n",
    "if __name__ == '__main__':\n",
    "    manager = mp.Manager()\n",
    "    lr3_return_dict = manager.dict()\n",
    "    jobs = []\n",
    "    for i in nodes_id:\n",
    "        \n",
    "        X_GB = big_sheet_GBR.iloc[end_nodes[i],:].drop(['id','bad'],axis=1)\n",
    "        Y_GB = big_sheet_GBR['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0})\n",
    "        X_NR = big_sheet_N.iloc[NR_nodes[i]]\n",
    "        \n",
    "        p = mp.Process(target=LR, args=(lr_clf1,X_GB,Y_GB,X_NR,i,lr3_return_dict))\n",
    "        \n",
    "#         if i in list1:\n",
    "#             p = mp.Process(target=LR, args=(clf1,X_GB,Y_GB,X_i,lr3_return_dict))\n",
    "#         else:\n",
    "#             p = mp.Process(target=LR, args=(clf2,X_GB,Y_GB,i,lr3_return_dict))\n",
    "\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for proc in jobs:\n",
    "        proc.join()\n",
    "        \n",
    "end = time.time()\n",
    "print('total time cost of duplicates processing takes {}seconds'.format(np.round(end-start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [auc,ks,y_test,pred[:,1],auc_train,ks_train,y_train,pred_train[:,1],\n",
    "#  used_fts,train_good,train_bad,test_good,test_bad,total_good,total_bad,\n",
    "#  coefs,NR_pred_df]\n",
    "\n",
    "auc,auc_train,i_s,ks,ks_train,num_samples = [],[],[],[],[],[]\n",
    "\n",
    "train_good,train_bad,test_good,test_bad,total_good,total_bad = [],[],[],[],[],[]\n",
    "\n",
    "used_fts = {}\n",
    "coefs = {}\n",
    "coefs['fts'] = mother_X.columns\n",
    "coefs['mother']=mother_coefs\n",
    "lr_preds_df = {}\n",
    "\n",
    "for i in lr3_return_dict.keys():\n",
    "    i_s.append(i)\n",
    "    auc.append(lr3_return_dict[i][0])\n",
    "    ks.append(lr3_return_dict[i][1])\n",
    "    auc_train.append(lr3_return_dict[i][4])\n",
    "    ks_train.append(lr3_return_dict[i][5])\n",
    "    num_samples.append(len(outside_data_tree3.end_nodes[i]))\n",
    "    train_good.append(lr3_return_dict[i][9])\n",
    "    train_bad.append(lr3_return_dict[i][10])\n",
    "    test_good.append(lr3_return_dict[i][11])\n",
    "    test_bad.append(lr3_return_dict[i][12])\n",
    "    total_good.append(lr3_return_dict[i][13])\n",
    "    total_bad.append(lr3_return_dict[i][14])\n",
    "    used_fts[i] = lr3_return_dict[i][8]\n",
    "    coefs[i]=lr3_return_dict[i][15]\n",
    "    lr_preds_df[i]=lr3_return_dict[i][-1]\n",
    "    \n",
    "i_s.append('mother')\n",
    "auc.append(mother_auc)\n",
    "ks.append(mother_ks)\n",
    "auc_train.append(mother_auc_train)\n",
    "ks_train.append(mother_ks_train)\n",
    "num_samples.append(393294)\n",
    "train_good.append(mother_train_good)\n",
    "train_bad.append(mother_train_bad)\n",
    "test_good.append(mother_test_good)\n",
    "test_bad.append(mother_test_bad)\n",
    "total_good.append(mother_total_good)\n",
    "total_bad.append(mother_total_bad)\n",
    "lr_preds_df['mother']=mother_lr_NR_pred\n",
    "\n",
    "y_test_stack = np.array([])\n",
    "y_pred_stack = np.array([])\n",
    "y_pred_train_stack = np.array([])\n",
    "y_train_stack = np.array([])\n",
    "for i in lr3_return_dict.keys():\n",
    "    y_test_stack = np.concatenate([y_test_stack,lr3_return_dict[i][2]])\n",
    "    y_pred_stack = np.concatenate([y_pred_stack,lr3_return_dict[i][3]])\n",
    "    y_pred_train_stack = np.concatenate([y_pred_train_stack,lr3_return_dict[i][7]])\n",
    "    y_train_stack = np.concatenate([y_train_stack,lr3_return_dict[i][6]])\n",
    "    \n",
    "total_auc = roc_auc_score(y_test_stack,y_pred_stack)\n",
    "fpr,tpr,thresholds= roc_curve(y_test_stack,y_pred_stack)\n",
    "total_ks = max(tpr-fpr)\n",
    "\n",
    "total_auc_train = roc_auc_score(y_train_stack,y_pred_train_stack)\n",
    "fpr,tpr,thresholds= roc_curve(y_train_stack,y_pred_train_stack)\n",
    "total_ks_train = max(tpr-fpr)\n",
    "\n",
    "i_s.append('子群整合')\n",
    "auc.append(total_auc)\n",
    "ks.append(total_ks)\n",
    "auc_train.append(total_auc_train)\n",
    "ks_train.append(total_ks_train)\n",
    "num_samples.append(393294)\n",
    "train_good.append(mother_train_good)\n",
    "train_bad.append(mother_train_bad)\n",
    "test_good.append(mother_test_good)\n",
    "test_bad.append(mother_test_bad)\n",
    "total_good.append(mother_total_good)\n",
    "total_bad.append(mother_total_bad)\n",
    "\n",
    "total_auc,total_ks,total_auc_train,total_ks_train\n",
    "lr3_result_df = pd.DataFrame({'子群ID':i_s,'test_good':test_good,'test_bad':test_bad,'test_auc':auc,'test_ks':ks,\n",
    "                              'train_good':train_good,'train_bad':train_bad,'auc_train':auc_train,'ks_train':ks_train,\n",
    "                              'total_good':total_good,'total_bad':total_bad,'num_sample':num_samples}).sort_values(by=['test_ks','test_auc'],ascending=False)\n",
    "\n",
    "# lr3_480_0423_result_df.to_csv('lr3_result_df_480_0423.csv')\n",
    "lr3_weights = pd.DataFrame(coefs)\n",
    "# lr3_480_weights.to_csv('lr3_weights_df_480_0423.csv',encoding='GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr3_result_df['train_test_gap'] = lr3_result_df['auc_train']-lr3_result_df['test_auc']\n",
    "lr3_result_df.sort_values(by='train_test_gap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lr_preds_df['mother']['pred_good_prob']<0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds_concated = pd.concat([lr_preds_df[3],lr_preds_df[4],lr_preds_df[10],lr_preds_df[11],\n",
    "                               lr_preds_df[6],lr_preds_df[14],lr_preds_df[13],lr_preds_df[7]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lr_preds_concated['pred_good_prob']<0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xgb_clf,auc,ks,y_test,pred[:,1],auc_train,ks_train,y_train,\n",
    "# pred_train[:,1],used_fts,train_good,train_bad,test_good,test_bad,total_good,total_bad\n",
    "mother_X = big_sheet_GB.drop(['id','bad'],axis=1)\n",
    "mother_Y = big_sheet_GB['bad'].replace({'G':1,'B':0})\n",
    "mother_N = big_sheet_N\n",
    "[lgbm_mother_auc,lgbm_mother_ks,lgbm_mother_y_test,lgbm_mother_pred,\n",
    " lgbm_mother_auc_train,lgbm_mother_ks_train,lgbm_mother_y_train,lgbm_mother_pred_train,\n",
    " lgbm_mother_train_good,lgbm_mother_train_bad,lgbm_mother_test_good,lgbm_mother_test_bad,\n",
    " lgbm_mother_total_good,lgbm_mother_total_bad,mother_lgbm_ftimp,\n",
    " lgbm_mother_NRpred,lgbm_mother_GBthres,mother_lgbm_clf] = LGBM(lgbm_clf0,mother_X,mother_Y,mother_N,'mother')\n",
    "mother_Y.value_counts(),lgbm_mother_auc,lgbm_mother_ks,lgbm_mother_auc_train,lgbm_mother_ks_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lgbm_mother_NRpred['pred_good_prob']<=(1-lgbm_mother_GBthres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_mother_NRpred.to_csv('lgbm_mother_0508.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to train LGBM modelpair5\n",
      "[1]\tvalid_0's binary_logloss: 0.328847\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.322179\n",
      "[3]\tvalid_0's binary_logloss: 0.316898\n",
      "[4]\tvalid_0's binary_logloss: 0.312826\n",
      "[5]\tvalid_0's binary_logloss: 0.309465\n",
      "[6]\tvalid_0's binary_logloss: 0.306548\n",
      "[7]\tvalid_0's binary_logloss: 0.303984\n",
      "[8]\tvalid_0's binary_logloss: 0.301722\n",
      "[9]\tvalid_0's binary_logloss: 0.29984\n",
      "[10]\tvalid_0's binary_logloss: 0.298146\n",
      "[11]\tvalid_0's binary_logloss: 0.296718\n",
      "[12]\tvalid_0's binary_logloss: 0.29537\n",
      "[13]\tvalid_0's binary_logloss: 0.294161\n",
      "[14]\tvalid_0's binary_logloss: 0.29295\n",
      "[15]\tvalid_0's binary_logloss: 0.291894\n",
      "[16]\tvalid_0's binary_logloss: 0.290825\n",
      "[17]\tvalid_0's binary_logloss: 0.289903\n",
      "[18]\tvalid_0's binary_logloss: 0.289146\n",
      "[19]\tvalid_0's binary_logloss: 0.28836\n",
      "[20]\tvalid_0's binary_logloss: 0.287723\n",
      "[21]\tvalid_0's binary_logloss: 0.28707\n",
      "[22]\tvalid_0's binary_logloss: 0.286441\n",
      "[23]\tvalid_0's binary_logloss: 0.28587\n",
      "[24]\tvalid_0's binary_logloss: 0.285306\n",
      "[25]\tvalid_0's binary_logloss: 0.284799\n",
      "[26]\tvalid_0's binary_logloss: 0.284325\n",
      "[27]\tvalid_0's binary_logloss: 0.283849\n",
      "[28]\tvalid_0's binary_logloss: 0.283453\n",
      "[29]\tvalid_0's binary_logloss: 0.283027\n",
      "[30]\tvalid_0's binary_logloss: 0.282637\n",
      "[31]\tvalid_0's binary_logloss: 0.282236\n",
      "[32]\tvalid_0's binary_logloss: 0.281922\n",
      "[33]\tvalid_0's binary_logloss: 0.281571\n",
      "[34]\tvalid_0's binary_logloss: 0.281233\n",
      "[35]\tvalid_0's binary_logloss: 0.280922\n",
      "[36]\tvalid_0's binary_logloss: 0.280558\n",
      "[37]\tvalid_0's binary_logloss: 0.280241\n",
      "[38]\tvalid_0's binary_logloss: 0.279973\n",
      "[39]\tvalid_0's binary_logloss: 0.279688\n",
      "[40]\tvalid_0's binary_logloss: 0.279294\n",
      "[41]\tvalid_0's binary_logloss: 0.279068\n",
      "[42]\tvalid_0's binary_logloss: 0.278886\n",
      "[43]\tvalid_0's binary_logloss: 0.278626\n",
      "[44]\tvalid_0's binary_logloss: 0.278473\n",
      "[45]\tvalid_0's binary_logloss: 0.27823\n",
      "[46]\tvalid_0's binary_logloss: 0.277991\n",
      "[47]\tvalid_0's binary_logloss: 0.277681\n",
      "[48]\tvalid_0's binary_logloss: 0.277519\n",
      "[49]\tvalid_0's binary_logloss: 0.277387\n",
      "[50]\tvalid_0's binary_logloss: 0.27718\n",
      "[51]\tvalid_0's binary_logloss: 0.276937\n",
      "[52]\tvalid_0's binary_logloss: 0.276803\n",
      "[53]\tvalid_0's binary_logloss: 0.276558\n",
      "[54]\tvalid_0's binary_logloss: 0.276307\n",
      "[55]\tvalid_0's binary_logloss: 0.276215\n",
      "[56]\tvalid_0's binary_logloss: 0.276085\n",
      "[57]\tvalid_0's binary_logloss: 0.275966\n",
      "[58]\tvalid_0's binary_logloss: 0.275809\n",
      "[59]\tvalid_0's binary_logloss: 0.2757\n",
      "[60]\tvalid_0's binary_logloss: 0.275584\n",
      "[61]\tvalid_0's binary_logloss: 0.275456\n",
      "[62]\tvalid_0's binary_logloss: 0.275335\n",
      "[63]\tvalid_0's binary_logloss: 0.275244\n",
      "[64]\tvalid_0's binary_logloss: 0.275123\n",
      "[65]\tvalid_0's binary_logloss: 0.275008\n",
      "[66]\tvalid_0's binary_logloss: 0.274847\n",
      "[67]\tvalid_0's binary_logloss: 0.274743\n",
      "[68]\tvalid_0's binary_logloss: 0.274661\n",
      "[69]\tvalid_0's binary_logloss: 0.274566\n",
      "[70]\tvalid_0's binary_logloss: 0.274457\n",
      "[71]\tvalid_0's binary_logloss: 0.274357\n",
      "[72]\tvalid_0's binary_logloss: 0.274296\n",
      "[73]\tvalid_0's binary_logloss: 0.274196\n",
      "[74]\tvalid_0's binary_logloss: 0.274145\n",
      "[75]\tvalid_0's binary_logloss: 0.274078\n",
      "[76]\tvalid_0's binary_logloss: 0.274003\n",
      "[77]\tvalid_0's binary_logloss: 0.273935\n",
      "[78]\tvalid_0's binary_logloss: 0.273869\n",
      "[79]\tvalid_0's binary_logloss: 0.273799\n",
      "[80]\tvalid_0's binary_logloss: 0.273656\n",
      "[81]\tvalid_0's binary_logloss: 0.273588\n",
      "[82]\tvalid_0's binary_logloss: 0.273527\n",
      "[83]\tvalid_0's binary_logloss: 0.273505\n",
      "[84]\tvalid_0's binary_logloss: 0.273474\n",
      "[85]\tvalid_0's binary_logloss: 0.273402\n",
      "[86]\tvalid_0's binary_logloss: 0.273395\n",
      "[87]\tvalid_0's binary_logloss: 0.273344\n",
      "[88]\tvalid_0's binary_logloss: 0.273302\n",
      "[89]\tvalid_0's binary_logloss: 0.273212\n",
      "[90]\tvalid_0's binary_logloss: 0.273123\n",
      "[91]\tvalid_0's binary_logloss: 0.273081\n",
      "[92]\tvalid_0's binary_logloss: 0.272979\n",
      "[93]\tvalid_0's binary_logloss: 0.272917\n",
      "[94]\tvalid_0's binary_logloss: 0.272858\n",
      "[95]\tvalid_0's binary_logloss: 0.272859\n",
      "[96]\tvalid_0's binary_logloss: 0.272812\n",
      "[97]\tvalid_0's binary_logloss: 0.272759\n",
      "[98]\tvalid_0's binary_logloss: 0.272673\n",
      "[99]\tvalid_0's binary_logloss: 0.272678\n",
      "[100]\tvalid_0's binary_logloss: 0.272654\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.272654\n",
      "[1]\tvalid_0's binary_logloss: 0.329043\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.322379\n",
      "[3]\tvalid_0's binary_logloss: 0.317451\n",
      "[4]\tvalid_0's binary_logloss: 0.313492\n",
      "[5]\tvalid_0's binary_logloss: 0.310296\n",
      "[6]\tvalid_0's binary_logloss: 0.307318\n",
      "[7]\tvalid_0's binary_logloss: 0.304788\n",
      "[8]\tvalid_0's binary_logloss: 0.30264\n",
      "[9]\tvalid_0's binary_logloss: 0.300723\n",
      "[10]\tvalid_0's binary_logloss: 0.298873\n",
      "[11]\tvalid_0's binary_logloss: 0.297485\n",
      "[12]\tvalid_0's binary_logloss: 0.296168\n",
      "[13]\tvalid_0's binary_logloss: 0.295064\n",
      "[14]\tvalid_0's binary_logloss: 0.294061\n",
      "[15]\tvalid_0's binary_logloss: 0.293055\n",
      "[16]\tvalid_0's binary_logloss: 0.2922\n",
      "[17]\tvalid_0's binary_logloss: 0.291245\n",
      "[18]\tvalid_0's binary_logloss: 0.290481\n",
      "[19]\tvalid_0's binary_logloss: 0.289669\n",
      "[20]\tvalid_0's binary_logloss: 0.289036\n",
      "[21]\tvalid_0's binary_logloss: 0.288402\n",
      "[22]\tvalid_0's binary_logloss: 0.287808\n",
      "[23]\tvalid_0's binary_logloss: 0.287072\n",
      "[24]\tvalid_0's binary_logloss: 0.286511\n",
      "[25]\tvalid_0's binary_logloss: 0.286005\n",
      "[26]\tvalid_0's binary_logloss: 0.28551\n",
      "[27]\tvalid_0's binary_logloss: 0.285083\n",
      "[28]\tvalid_0's binary_logloss: 0.284734\n",
      "[29]\tvalid_0's binary_logloss: 0.284372\n",
      "[30]\tvalid_0's binary_logloss: 0.28397\n",
      "[31]\tvalid_0's binary_logloss: 0.283647\n",
      "[32]\tvalid_0's binary_logloss: 0.283295\n",
      "[33]\tvalid_0's binary_logloss: 0.282949\n",
      "[34]\tvalid_0's binary_logloss: 0.282483\n",
      "[35]\tvalid_0's binary_logloss: 0.282179\n",
      "[36]\tvalid_0's binary_logloss: 0.281841\n",
      "[37]\tvalid_0's binary_logloss: 0.281629\n",
      "[38]\tvalid_0's binary_logloss: 0.281336\n",
      "[39]\tvalid_0's binary_logloss: 0.280881\n",
      "[40]\tvalid_0's binary_logloss: 0.280616\n",
      "[41]\tvalid_0's binary_logloss: 0.280376\n",
      "[42]\tvalid_0's binary_logloss: 0.280169\n",
      "[43]\tvalid_0's binary_logloss: 0.27991\n",
      "[44]\tvalid_0's binary_logloss: 0.279709\n",
      "[45]\tvalid_0's binary_logloss: 0.279481\n",
      "[46]\tvalid_0's binary_logloss: 0.279276\n",
      "[47]\tvalid_0's binary_logloss: 0.279099\n",
      "[48]\tvalid_0's binary_logloss: 0.278827\n",
      "[49]\tvalid_0's binary_logloss: 0.278683\n",
      "[50]\tvalid_0's binary_logloss: 0.27848\n",
      "[51]\tvalid_0's binary_logloss: 0.278305\n",
      "[52]\tvalid_0's binary_logloss: 0.278091\n",
      "[53]\tvalid_0's binary_logloss: 0.277942\n",
      "[54]\tvalid_0's binary_logloss: 0.277804\n",
      "[55]\tvalid_0's binary_logloss: 0.27765\n",
      "[56]\tvalid_0's binary_logloss: 0.277479\n",
      "[57]\tvalid_0's binary_logloss: 0.277363\n",
      "[58]\tvalid_0's binary_logloss: 0.27727\n",
      "[59]\tvalid_0's binary_logloss: 0.27717\n",
      "[60]\tvalid_0's binary_logloss: 0.277064\n",
      "[61]\tvalid_0's binary_logloss: 0.27694\n",
      "[62]\tvalid_0's binary_logloss: 0.276786\n",
      "[63]\tvalid_0's binary_logloss: 0.27671\n",
      "[64]\tvalid_0's binary_logloss: 0.276508\n",
      "[65]\tvalid_0's binary_logloss: 0.276418\n",
      "[66]\tvalid_0's binary_logloss: 0.276252\n",
      "[67]\tvalid_0's binary_logloss: 0.276185\n",
      "[68]\tvalid_0's binary_logloss: 0.276111\n",
      "[69]\tvalid_0's binary_logloss: 0.275983\n",
      "[70]\tvalid_0's binary_logloss: 0.275936\n",
      "[71]\tvalid_0's binary_logloss: 0.275871\n",
      "[72]\tvalid_0's binary_logloss: 0.275794\n",
      "[73]\tvalid_0's binary_logloss: 0.275726\n",
      "[74]\tvalid_0's binary_logloss: 0.275648\n",
      "[75]\tvalid_0's binary_logloss: 0.275587\n",
      "[76]\tvalid_0's binary_logloss: 0.275494\n",
      "[77]\tvalid_0's binary_logloss: 0.275423\n",
      "[78]\tvalid_0's binary_logloss: 0.275338\n",
      "[79]\tvalid_0's binary_logloss: 0.275289\n",
      "[80]\tvalid_0's binary_logloss: 0.275254\n",
      "[81]\tvalid_0's binary_logloss: 0.275137\n",
      "[82]\tvalid_0's binary_logloss: 0.275093\n",
      "[83]\tvalid_0's binary_logloss: 0.274987\n",
      "[84]\tvalid_0's binary_logloss: 0.274914\n",
      "[85]\tvalid_0's binary_logloss: 0.27487\n",
      "[86]\tvalid_0's binary_logloss: 0.274773\n",
      "[87]\tvalid_0's binary_logloss: 0.274716\n",
      "[88]\tvalid_0's binary_logloss: 0.274702\n",
      "[89]\tvalid_0's binary_logloss: 0.274668\n",
      "[90]\tvalid_0's binary_logloss: 0.274644\n",
      "[91]\tvalid_0's binary_logloss: 0.274597\n",
      "[92]\tvalid_0's binary_logloss: 0.27457\n",
      "[93]\tvalid_0's binary_logloss: 0.274502\n",
      "[94]\tvalid_0's binary_logloss: 0.274456\n",
      "[95]\tvalid_0's binary_logloss: 0.274425\n",
      "[96]\tvalid_0's binary_logloss: 0.274354\n",
      "[97]\tvalid_0's binary_logloss: 0.274344\n",
      "[98]\tvalid_0's binary_logloss: 0.274324\n",
      "[99]\tvalid_0's binary_logloss: 0.274305\n",
      "[100]\tvalid_0's binary_logloss: 0.274251\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.274251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.328885\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.322243\n",
      "[3]\tvalid_0's binary_logloss: 0.317225\n",
      "[4]\tvalid_0's binary_logloss: 0.313094\n",
      "[5]\tvalid_0's binary_logloss: 0.309695\n",
      "[6]\tvalid_0's binary_logloss: 0.306906\n",
      "[7]\tvalid_0's binary_logloss: 0.304408\n",
      "[8]\tvalid_0's binary_logloss: 0.302277\n",
      "[9]\tvalid_0's binary_logloss: 0.300314\n",
      "[10]\tvalid_0's binary_logloss: 0.298728\n",
      "[11]\tvalid_0's binary_logloss: 0.297282\n",
      "[12]\tvalid_0's binary_logloss: 0.295957\n",
      "[13]\tvalid_0's binary_logloss: 0.29469\n",
      "[14]\tvalid_0's binary_logloss: 0.293615\n",
      "[15]\tvalid_0's binary_logloss: 0.292549\n",
      "[16]\tvalid_0's binary_logloss: 0.291723\n",
      "[17]\tvalid_0's binary_logloss: 0.290777\n",
      "[18]\tvalid_0's binary_logloss: 0.29001\n",
      "[19]\tvalid_0's binary_logloss: 0.289143\n",
      "[20]\tvalid_0's binary_logloss: 0.288435\n",
      "[21]\tvalid_0's binary_logloss: 0.287742\n",
      "[22]\tvalid_0's binary_logloss: 0.28719\n",
      "[23]\tvalid_0's binary_logloss: 0.286653\n",
      "[24]\tvalid_0's binary_logloss: 0.286098\n",
      "[25]\tvalid_0's binary_logloss: 0.28555\n",
      "[26]\tvalid_0's binary_logloss: 0.285118\n",
      "[27]\tvalid_0's binary_logloss: 0.284626\n",
      "[28]\tvalid_0's binary_logloss: 0.284182\n",
      "[29]\tvalid_0's binary_logloss: 0.283694\n",
      "[30]\tvalid_0's binary_logloss: 0.283316\n",
      "[31]\tvalid_0's binary_logloss: 0.282827\n",
      "[32]\tvalid_0's binary_logloss: 0.282492\n",
      "[33]\tvalid_0's binary_logloss: 0.282139\n",
      "[34]\tvalid_0's binary_logloss: 0.281705\n",
      "[35]\tvalid_0's binary_logloss: 0.281367\n",
      "[36]\tvalid_0's binary_logloss: 0.281049\n",
      "[37]\tvalid_0's binary_logloss: 0.280779\n",
      "[38]\tvalid_0's binary_logloss: 0.280447\n",
      "[39]\tvalid_0's binary_logloss: 0.28003\n",
      "[40]\tvalid_0's binary_logloss: 0.279829\n",
      "[41]\tvalid_0's binary_logloss: 0.279511\n",
      "[42]\tvalid_0's binary_logloss: 0.279268\n",
      "[43]\tvalid_0's binary_logloss: 0.279059\n",
      "[44]\tvalid_0's binary_logloss: 0.278793\n",
      "[45]\tvalid_0's binary_logloss: 0.278602\n",
      "[46]\tvalid_0's binary_logloss: 0.278385\n",
      "[47]\tvalid_0's binary_logloss: 0.278206\n",
      "[48]\tvalid_0's binary_logloss: 0.27801\n",
      "[49]\tvalid_0's binary_logloss: 0.277849\n",
      "[50]\tvalid_0's binary_logloss: 0.277633\n",
      "[51]\tvalid_0's binary_logloss: 0.277414\n",
      "[52]\tvalid_0's binary_logloss: 0.277242\n",
      "[53]\tvalid_0's binary_logloss: 0.277094\n",
      "[54]\tvalid_0's binary_logloss: 0.276909\n",
      "[55]\tvalid_0's binary_logloss: 0.276736\n",
      "[56]\tvalid_0's binary_logloss: 0.276509\n",
      "[57]\tvalid_0's binary_logloss: 0.276376\n",
      "[58]\tvalid_0's binary_logloss: 0.276251\n",
      "[59]\tvalid_0's binary_logloss: 0.276084\n",
      "[60]\tvalid_0's binary_logloss: 0.275966\n",
      "[61]\tvalid_0's binary_logloss: 0.275848\n",
      "[62]\tvalid_0's binary_logloss: 0.275779\n",
      "[63]\tvalid_0's binary_logloss: 0.275609\n",
      "[64]\tvalid_0's binary_logloss: 0.275483\n",
      "[65]\tvalid_0's binary_logloss: 0.275374\n",
      "[66]\tvalid_0's binary_logloss: 0.275269\n",
      "[67]\tvalid_0's binary_logloss: 0.275184\n",
      "[68]\tvalid_0's binary_logloss: 0.275072\n",
      "[69]\tvalid_0's binary_logloss: 0.275003\n",
      "[70]\tvalid_0's binary_logloss: 0.274891\n",
      "[71]\tvalid_0's binary_logloss: 0.274764\n",
      "[72]\tvalid_0's binary_logloss: 0.274706\n",
      "[73]\tvalid_0's binary_logloss: 0.274549\n",
      "[74]\tvalid_0's binary_logloss: 0.274488\n",
      "[75]\tvalid_0's binary_logloss: 0.274403\n",
      "[76]\tvalid_0's binary_logloss: 0.274279\n",
      "[77]\tvalid_0's binary_logloss: 0.274142\n",
      "[78]\tvalid_0's binary_logloss: 0.27408\n",
      "[79]\tvalid_0's binary_logloss: 0.274018\n",
      "[80]\tvalid_0's binary_logloss: 0.273949\n",
      "[81]\tvalid_0's binary_logloss: 0.273923\n",
      "[82]\tvalid_0's binary_logloss: 0.273812\n",
      "[83]\tvalid_0's binary_logloss: 0.273734\n",
      "[84]\tvalid_0's binary_logloss: 0.273691\n",
      "[85]\tvalid_0's binary_logloss: 0.273641\n",
      "[86]\tvalid_0's binary_logloss: 0.273573\n",
      "[87]\tvalid_0's binary_logloss: 0.273503\n",
      "[88]\tvalid_0's binary_logloss: 0.273428\n",
      "[89]\tvalid_0's binary_logloss: 0.273357\n",
      "[90]\tvalid_0's binary_logloss: 0.273282\n",
      "[91]\tvalid_0's binary_logloss: 0.273244\n",
      "[92]\tvalid_0's binary_logloss: 0.273155\n",
      "[93]\tvalid_0's binary_logloss: 0.273107\n",
      "[94]\tvalid_0's binary_logloss: 0.273075\n",
      "[95]\tvalid_0's binary_logloss: 0.273009\n",
      "[96]\tvalid_0's binary_logloss: 0.272988\n",
      "[97]\tvalid_0's binary_logloss: 0.272974\n",
      "[98]\tvalid_0's binary_logloss: 0.272933\n",
      "[99]\tvalid_0's binary_logloss: 0.272884\n",
      "[100]\tvalid_0's binary_logloss: 0.272857\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.272857\n",
      "[1]\tvalid_0's binary_logloss: 0.32927\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.322751\n",
      "[3]\tvalid_0's binary_logloss: 0.317801\n",
      "[4]\tvalid_0's binary_logloss: 0.313805\n",
      "[5]\tvalid_0's binary_logloss: 0.310231\n",
      "[6]\tvalid_0's binary_logloss: 0.307261\n",
      "[7]\tvalid_0's binary_logloss: 0.304859\n",
      "[8]\tvalid_0's binary_logloss: 0.302838\n",
      "[9]\tvalid_0's binary_logloss: 0.300766\n",
      "[10]\tvalid_0's binary_logloss: 0.299097\n",
      "[11]\tvalid_0's binary_logloss: 0.297623\n",
      "[12]\tvalid_0's binary_logloss: 0.296279\n",
      "[13]\tvalid_0's binary_logloss: 0.295103\n",
      "[14]\tvalid_0's binary_logloss: 0.293958\n",
      "[15]\tvalid_0's binary_logloss: 0.293028\n",
      "[16]\tvalid_0's binary_logloss: 0.292015\n",
      "[17]\tvalid_0's binary_logloss: 0.291021\n",
      "[18]\tvalid_0's binary_logloss: 0.290234\n",
      "[19]\tvalid_0's binary_logloss: 0.289488\n",
      "[20]\tvalid_0's binary_logloss: 0.288744\n",
      "[21]\tvalid_0's binary_logloss: 0.288098\n",
      "[22]\tvalid_0's binary_logloss: 0.287503\n",
      "[23]\tvalid_0's binary_logloss: 0.286879\n",
      "[24]\tvalid_0's binary_logloss: 0.286312\n",
      "[25]\tvalid_0's binary_logloss: 0.285769\n",
      "[26]\tvalid_0's binary_logloss: 0.285283\n",
      "[27]\tvalid_0's binary_logloss: 0.284819\n",
      "[28]\tvalid_0's binary_logloss: 0.284434\n",
      "[29]\tvalid_0's binary_logloss: 0.284034\n",
      "[30]\tvalid_0's binary_logloss: 0.283553\n",
      "[31]\tvalid_0's binary_logloss: 0.283178\n",
      "[32]\tvalid_0's binary_logloss: 0.28281\n",
      "[33]\tvalid_0's binary_logloss: 0.282453\n",
      "[34]\tvalid_0's binary_logloss: 0.282036\n",
      "[35]\tvalid_0's binary_logloss: 0.281761\n",
      "[36]\tvalid_0's binary_logloss: 0.281416\n",
      "[37]\tvalid_0's binary_logloss: 0.281161\n",
      "[38]\tvalid_0's binary_logloss: 0.280921\n",
      "[39]\tvalid_0's binary_logloss: 0.280651\n",
      "[40]\tvalid_0's binary_logloss: 0.280342\n",
      "[41]\tvalid_0's binary_logloss: 0.280067\n",
      "[42]\tvalid_0's binary_logloss: 0.279767\n",
      "[43]\tvalid_0's binary_logloss: 0.279533\n",
      "[44]\tvalid_0's binary_logloss: 0.279215\n",
      "[45]\tvalid_0's binary_logloss: 0.279053\n",
      "[46]\tvalid_0's binary_logloss: 0.278751\n",
      "[47]\tvalid_0's binary_logloss: 0.278552\n",
      "[48]\tvalid_0's binary_logloss: 0.278353\n",
      "[49]\tvalid_0's binary_logloss: 0.278168\n",
      "[50]\tvalid_0's binary_logloss: 0.277974\n",
      "[51]\tvalid_0's binary_logloss: 0.277794\n",
      "[52]\tvalid_0's binary_logloss: 0.277635\n",
      "[53]\tvalid_0's binary_logloss: 0.277498\n",
      "[54]\tvalid_0's binary_logloss: 0.277329\n",
      "[55]\tvalid_0's binary_logloss: 0.277187\n",
      "[56]\tvalid_0's binary_logloss: 0.277049\n",
      "[57]\tvalid_0's binary_logloss: 0.276877\n",
      "[58]\tvalid_0's binary_logloss: 0.276668\n",
      "[59]\tvalid_0's binary_logloss: 0.276516\n",
      "[60]\tvalid_0's binary_logloss: 0.276434\n",
      "[61]\tvalid_0's binary_logloss: 0.276257\n",
      "[62]\tvalid_0's binary_logloss: 0.276119\n",
      "[63]\tvalid_0's binary_logloss: 0.276051\n",
      "[64]\tvalid_0's binary_logloss: 0.275973\n",
      "[65]\tvalid_0's binary_logloss: 0.27586\n",
      "[66]\tvalid_0's binary_logloss: 0.27577\n",
      "[67]\tvalid_0's binary_logloss: 0.275656\n",
      "[68]\tvalid_0's binary_logloss: 0.275619\n",
      "[69]\tvalid_0's binary_logloss: 0.275521\n",
      "[70]\tvalid_0's binary_logloss: 0.275425\n",
      "[71]\tvalid_0's binary_logloss: 0.275343\n",
      "[72]\tvalid_0's binary_logloss: 0.275241\n",
      "[73]\tvalid_0's binary_logloss: 0.275204\n",
      "[74]\tvalid_0's binary_logloss: 0.275138\n",
      "[75]\tvalid_0's binary_logloss: 0.275055\n",
      "[76]\tvalid_0's binary_logloss: 0.275004\n",
      "[77]\tvalid_0's binary_logloss: 0.274958\n",
      "[78]\tvalid_0's binary_logloss: 0.274835\n",
      "[79]\tvalid_0's binary_logloss: 0.274796\n",
      "[80]\tvalid_0's binary_logloss: 0.27473\n",
      "[81]\tvalid_0's binary_logloss: 0.274602\n",
      "[82]\tvalid_0's binary_logloss: 0.274571\n",
      "[83]\tvalid_0's binary_logloss: 0.274511\n",
      "[84]\tvalid_0's binary_logloss: 0.274461\n",
      "[85]\tvalid_0's binary_logloss: 0.274411\n",
      "[86]\tvalid_0's binary_logloss: 0.274307\n",
      "[87]\tvalid_0's binary_logloss: 0.274263\n",
      "[88]\tvalid_0's binary_logloss: 0.274159\n",
      "[89]\tvalid_0's binary_logloss: 0.274046\n",
      "[90]\tvalid_0's binary_logloss: 0.273997\n",
      "[91]\tvalid_0's binary_logloss: 0.273942\n",
      "[92]\tvalid_0's binary_logloss: 0.273911\n",
      "[93]\tvalid_0's binary_logloss: 0.273885\n",
      "[94]\tvalid_0's binary_logloss: 0.273859\n",
      "[95]\tvalid_0's binary_logloss: 0.273797\n",
      "[96]\tvalid_0's binary_logloss: 0.273748\n",
      "[97]\tvalid_0's binary_logloss: 0.2737\n",
      "[98]\tvalid_0's binary_logloss: 0.273633\n",
      "[99]\tvalid_0's binary_logloss: 0.27356\n",
      "[100]\tvalid_0's binary_logloss: 0.273555\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.273555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.328561\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.321721\n",
      "[3]\tvalid_0's binary_logloss: 0.316447\n",
      "[4]\tvalid_0's binary_logloss: 0.312214\n",
      "[5]\tvalid_0's binary_logloss: 0.30869\n",
      "[6]\tvalid_0's binary_logloss: 0.305876\n",
      "[7]\tvalid_0's binary_logloss: 0.30341\n",
      "[8]\tvalid_0's binary_logloss: 0.301211\n",
      "[9]\tvalid_0's binary_logloss: 0.299243\n",
      "[10]\tvalid_0's binary_logloss: 0.297599\n",
      "[11]\tvalid_0's binary_logloss: 0.296069\n",
      "[12]\tvalid_0's binary_logloss: 0.294727\n",
      "[13]\tvalid_0's binary_logloss: 0.29352\n",
      "[14]\tvalid_0's binary_logloss: 0.29229\n",
      "[15]\tvalid_0's binary_logloss: 0.29108\n",
      "[16]\tvalid_0's binary_logloss: 0.290104\n",
      "[17]\tvalid_0's binary_logloss: 0.289178\n",
      "[18]\tvalid_0's binary_logloss: 0.288342\n",
      "[19]\tvalid_0's binary_logloss: 0.287522\n",
      "[20]\tvalid_0's binary_logloss: 0.286837\n",
      "[21]\tvalid_0's binary_logloss: 0.286155\n",
      "[22]\tvalid_0's binary_logloss: 0.285443\n",
      "[23]\tvalid_0's binary_logloss: 0.284853\n",
      "[24]\tvalid_0's binary_logloss: 0.284354\n",
      "[25]\tvalid_0's binary_logloss: 0.283899\n",
      "[26]\tvalid_0's binary_logloss: 0.283343\n",
      "[27]\tvalid_0's binary_logloss: 0.282772\n",
      "[28]\tvalid_0's binary_logloss: 0.282353\n",
      "[29]\tvalid_0's binary_logloss: 0.281953\n",
      "[30]\tvalid_0's binary_logloss: 0.281551\n",
      "[31]\tvalid_0's binary_logloss: 0.28113\n",
      "[32]\tvalid_0's binary_logloss: 0.28067\n",
      "[33]\tvalid_0's binary_logloss: 0.280361\n",
      "[34]\tvalid_0's binary_logloss: 0.280001\n",
      "[35]\tvalid_0's binary_logloss: 0.279607\n",
      "[36]\tvalid_0's binary_logloss: 0.279265\n",
      "[37]\tvalid_0's binary_logloss: 0.278966\n",
      "[38]\tvalid_0's binary_logloss: 0.278676\n",
      "[39]\tvalid_0's binary_logloss: 0.278432\n",
      "[40]\tvalid_0's binary_logloss: 0.278131\n",
      "[41]\tvalid_0's binary_logloss: 0.2779\n",
      "[42]\tvalid_0's binary_logloss: 0.277634\n",
      "[43]\tvalid_0's binary_logloss: 0.277403\n",
      "[44]\tvalid_0's binary_logloss: 0.277163\n",
      "[45]\tvalid_0's binary_logloss: 0.276944\n",
      "[46]\tvalid_0's binary_logloss: 0.276742\n",
      "[47]\tvalid_0's binary_logloss: 0.276435\n",
      "[48]\tvalid_0's binary_logloss: 0.2763\n",
      "[49]\tvalid_0's binary_logloss: 0.27617\n",
      "[50]\tvalid_0's binary_logloss: 0.275983\n",
      "[51]\tvalid_0's binary_logloss: 0.275854\n",
      "[52]\tvalid_0's binary_logloss: 0.275684\n",
      "[53]\tvalid_0's binary_logloss: 0.275438\n",
      "[54]\tvalid_0's binary_logloss: 0.275247\n",
      "[55]\tvalid_0's binary_logloss: 0.275105\n",
      "[56]\tvalid_0's binary_logloss: 0.274986\n",
      "[57]\tvalid_0's binary_logloss: 0.274861\n",
      "[58]\tvalid_0's binary_logloss: 0.274772\n",
      "[59]\tvalid_0's binary_logloss: 0.274615\n",
      "[60]\tvalid_0's binary_logloss: 0.274411\n",
      "[61]\tvalid_0's binary_logloss: 0.274333\n",
      "[62]\tvalid_0's binary_logloss: 0.274209\n",
      "[63]\tvalid_0's binary_logloss: 0.274103\n",
      "[64]\tvalid_0's binary_logloss: 0.273999\n",
      "[65]\tvalid_0's binary_logloss: 0.273878\n",
      "[66]\tvalid_0's binary_logloss: 0.273827\n",
      "[67]\tvalid_0's binary_logloss: 0.273679\n",
      "[68]\tvalid_0's binary_logloss: 0.27362\n",
      "[69]\tvalid_0's binary_logloss: 0.273544\n",
      "[70]\tvalid_0's binary_logloss: 0.273469\n",
      "[71]\tvalid_0's binary_logloss: 0.273392\n",
      "[72]\tvalid_0's binary_logloss: 0.273301\n",
      "[73]\tvalid_0's binary_logloss: 0.273171\n",
      "[74]\tvalid_0's binary_logloss: 0.27313\n",
      "[75]\tvalid_0's binary_logloss: 0.273024\n",
      "[76]\tvalid_0's binary_logloss: 0.272958\n",
      "[77]\tvalid_0's binary_logloss: 0.272887\n",
      "[78]\tvalid_0's binary_logloss: 0.272826\n",
      "[79]\tvalid_0's binary_logloss: 0.27271\n",
      "[80]\tvalid_0's binary_logloss: 0.272627\n",
      "[81]\tvalid_0's binary_logloss: 0.27258\n",
      "[82]\tvalid_0's binary_logloss: 0.272535\n",
      "[83]\tvalid_0's binary_logloss: 0.272419\n",
      "[84]\tvalid_0's binary_logloss: 0.272386\n",
      "[85]\tvalid_0's binary_logloss: 0.272353\n",
      "[86]\tvalid_0's binary_logloss: 0.272291\n",
      "[87]\tvalid_0's binary_logloss: 0.272258\n",
      "[88]\tvalid_0's binary_logloss: 0.272192\n",
      "[89]\tvalid_0's binary_logloss: 0.27215\n",
      "[90]\tvalid_0's binary_logloss: 0.272102\n",
      "[91]\tvalid_0's binary_logloss: 0.272016\n",
      "[92]\tvalid_0's binary_logloss: 0.271991\n",
      "[93]\tvalid_0's binary_logloss: 0.271955\n",
      "[94]\tvalid_0's binary_logloss: 0.271915\n",
      "[95]\tvalid_0's binary_logloss: 0.271868\n",
      "[96]\tvalid_0's binary_logloss: 0.271842\n",
      "[97]\tvalid_0's binary_logloss: 0.271832\n",
      "[98]\tvalid_0's binary_logloss: 0.27182\n",
      "[99]\tvalid_0's binary_logloss: 0.271796\n",
      "[100]\tvalid_0's binary_logloss: 0.27177\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.27177\n",
      "LGBM modelpair5 finished, time spent 481.8seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_recurrent_month = {}\n",
    "\n",
    "X_GB = big_sheet_GB.drop(['id','bad'],axis=1)\n",
    "Y_GB = big_sheet_GB['bad'].replace({'G':1,'B':0})\n",
    "\n",
    "lgbm_clf0_0 = lgb.LGBMClassifier()\n",
    "lgbm_recurrent_month['pair5'] = LGBM(lgbm_clf0_0,X_GB,Y_GB,big_sheet_N,'pair5',if_month=1)\n",
    "\n",
    "# for i in month_pair_keys:\n",
    "    \n",
    "#     start = time.time()\n",
    "    \n",
    "#     lgbm_clf0_0 = lgb.LGBMClassifier()\n",
    "#     lgbm_recurrent_month[i] = LGBM(lgbm_clf0_0,X_GB,Y_GB,big_sheet_N,i,if_month=1)\n",
    "    \n",
    "#     end = time.time()\n",
    "#     print('{} model finished, time cost {}'.format(i,np.round(end-start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to train LGBM modelpair6\n",
      "[1]\tvalid_0's binary_logloss: 0.305964\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.297167\n",
      "[3]\tvalid_0's binary_logloss: 0.290877\n",
      "[4]\tvalid_0's binary_logloss: 0.285677\n",
      "[5]\tvalid_0's binary_logloss: 0.281654\n",
      "[6]\tvalid_0's binary_logloss: 0.278038\n",
      "[7]\tvalid_0's binary_logloss: 0.27508\n",
      "[8]\tvalid_0's binary_logloss: 0.272485\n",
      "[9]\tvalid_0's binary_logloss: 0.270333\n",
      "[10]\tvalid_0's binary_logloss: 0.26837\n",
      "[11]\tvalid_0's binary_logloss: 0.266708\n",
      "[12]\tvalid_0's binary_logloss: 0.265161\n",
      "[13]\tvalid_0's binary_logloss: 0.263903\n",
      "[14]\tvalid_0's binary_logloss: 0.262737\n",
      "[15]\tvalid_0's binary_logloss: 0.261614\n",
      "[16]\tvalid_0's binary_logloss: 0.26065\n",
      "[17]\tvalid_0's binary_logloss: 0.259738\n",
      "[18]\tvalid_0's binary_logloss: 0.258886\n",
      "[19]\tvalid_0's binary_logloss: 0.258191\n",
      "[20]\tvalid_0's binary_logloss: 0.257505\n",
      "[21]\tvalid_0's binary_logloss: 0.25685\n",
      "[22]\tvalid_0's binary_logloss: 0.25623\n",
      "[23]\tvalid_0's binary_logloss: 0.25567\n",
      "[24]\tvalid_0's binary_logloss: 0.255137\n",
      "[25]\tvalid_0's binary_logloss: 0.254682\n",
      "[26]\tvalid_0's binary_logloss: 0.254216\n",
      "[27]\tvalid_0's binary_logloss: 0.253728\n",
      "[28]\tvalid_0's binary_logloss: 0.253301\n",
      "[29]\tvalid_0's binary_logloss: 0.252889\n",
      "[30]\tvalid_0's binary_logloss: 0.25252\n",
      "[31]\tvalid_0's binary_logloss: 0.252169\n",
      "[32]\tvalid_0's binary_logloss: 0.251874\n",
      "[33]\tvalid_0's binary_logloss: 0.25161\n",
      "[34]\tvalid_0's binary_logloss: 0.251265\n",
      "[35]\tvalid_0's binary_logloss: 0.250987\n",
      "[36]\tvalid_0's binary_logloss: 0.250681\n",
      "[37]\tvalid_0's binary_logloss: 0.250305\n",
      "[38]\tvalid_0's binary_logloss: 0.250048\n",
      "[39]\tvalid_0's binary_logloss: 0.249782\n",
      "[40]\tvalid_0's binary_logloss: 0.249535\n",
      "[41]\tvalid_0's binary_logloss: 0.249347\n",
      "[42]\tvalid_0's binary_logloss: 0.249171\n",
      "[43]\tvalid_0's binary_logloss: 0.248939\n",
      "[44]\tvalid_0's binary_logloss: 0.248717\n",
      "[45]\tvalid_0's binary_logloss: 0.248483\n",
      "[46]\tvalid_0's binary_logloss: 0.248279\n",
      "[47]\tvalid_0's binary_logloss: 0.248081\n",
      "[48]\tvalid_0's binary_logloss: 0.247952\n",
      "[49]\tvalid_0's binary_logloss: 0.247815\n",
      "[50]\tvalid_0's binary_logloss: 0.24766\n",
      "[51]\tvalid_0's binary_logloss: 0.247436\n",
      "[52]\tvalid_0's binary_logloss: 0.247304\n",
      "[53]\tvalid_0's binary_logloss: 0.247221\n",
      "[54]\tvalid_0's binary_logloss: 0.246979\n",
      "[55]\tvalid_0's binary_logloss: 0.246816\n",
      "[56]\tvalid_0's binary_logloss: 0.246682\n",
      "[57]\tvalid_0's binary_logloss: 0.246555\n",
      "[58]\tvalid_0's binary_logloss: 0.246449\n",
      "[59]\tvalid_0's binary_logloss: 0.246344\n",
      "[60]\tvalid_0's binary_logloss: 0.246219\n",
      "[61]\tvalid_0's binary_logloss: 0.24611\n",
      "[62]\tvalid_0's binary_logloss: 0.246006\n",
      "[63]\tvalid_0's binary_logloss: 0.245858\n",
      "[64]\tvalid_0's binary_logloss: 0.245798\n",
      "[65]\tvalid_0's binary_logloss: 0.245731\n",
      "[66]\tvalid_0's binary_logloss: 0.245629\n",
      "[67]\tvalid_0's binary_logloss: 0.245507\n",
      "[68]\tvalid_0's binary_logloss: 0.245432\n",
      "[69]\tvalid_0's binary_logloss: 0.245337\n",
      "[70]\tvalid_0's binary_logloss: 0.24521\n",
      "[71]\tvalid_0's binary_logloss: 0.245134\n",
      "[72]\tvalid_0's binary_logloss: 0.24503\n",
      "[73]\tvalid_0's binary_logloss: 0.244971\n",
      "[74]\tvalid_0's binary_logloss: 0.244918\n",
      "[75]\tvalid_0's binary_logloss: 0.244847\n",
      "[76]\tvalid_0's binary_logloss: 0.24475\n",
      "[77]\tvalid_0's binary_logloss: 0.244666\n",
      "[78]\tvalid_0's binary_logloss: 0.244615\n",
      "[79]\tvalid_0's binary_logloss: 0.244555\n",
      "[80]\tvalid_0's binary_logloss: 0.244515\n",
      "[81]\tvalid_0's binary_logloss: 0.24447\n",
      "[82]\tvalid_0's binary_logloss: 0.24443\n",
      "[83]\tvalid_0's binary_logloss: 0.244358\n",
      "[84]\tvalid_0's binary_logloss: 0.244344\n",
      "[85]\tvalid_0's binary_logloss: 0.244327\n",
      "[86]\tvalid_0's binary_logloss: 0.244276\n",
      "[87]\tvalid_0's binary_logloss: 0.244229\n",
      "[88]\tvalid_0's binary_logloss: 0.24422\n",
      "[89]\tvalid_0's binary_logloss: 0.244169\n",
      "[90]\tvalid_0's binary_logloss: 0.244118\n",
      "[91]\tvalid_0's binary_logloss: 0.244094\n",
      "[92]\tvalid_0's binary_logloss: 0.244048\n",
      "[93]\tvalid_0's binary_logloss: 0.243987\n",
      "[94]\tvalid_0's binary_logloss: 0.243947\n",
      "[95]\tvalid_0's binary_logloss: 0.243883\n",
      "[96]\tvalid_0's binary_logloss: 0.243872\n",
      "[97]\tvalid_0's binary_logloss: 0.243856\n",
      "[98]\tvalid_0's binary_logloss: 0.243835\n",
      "[99]\tvalid_0's binary_logloss: 0.24379\n",
      "[100]\tvalid_0's binary_logloss: 0.243737\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.243737\n",
      "[1]\tvalid_0's binary_logloss: 0.306405\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.297933\n",
      "[3]\tvalid_0's binary_logloss: 0.291621\n",
      "[4]\tvalid_0's binary_logloss: 0.286481\n",
      "[5]\tvalid_0's binary_logloss: 0.282375\n",
      "[6]\tvalid_0's binary_logloss: 0.278931\n",
      "[7]\tvalid_0's binary_logloss: 0.27595\n",
      "[8]\tvalid_0's binary_logloss: 0.273489\n",
      "[9]\tvalid_0's binary_logloss: 0.271271\n",
      "[10]\tvalid_0's binary_logloss: 0.269463\n",
      "[11]\tvalid_0's binary_logloss: 0.267849\n",
      "[12]\tvalid_0's binary_logloss: 0.266482\n",
      "[13]\tvalid_0's binary_logloss: 0.265068\n",
      "[14]\tvalid_0's binary_logloss: 0.263987\n",
      "[15]\tvalid_0's binary_logloss: 0.262858\n",
      "[16]\tvalid_0's binary_logloss: 0.261873\n",
      "[17]\tvalid_0's binary_logloss: 0.260903\n",
      "[18]\tvalid_0's binary_logloss: 0.260102\n",
      "[19]\tvalid_0's binary_logloss: 0.259384\n",
      "[20]\tvalid_0's binary_logloss: 0.258745\n",
      "[21]\tvalid_0's binary_logloss: 0.258047\n",
      "[22]\tvalid_0's binary_logloss: 0.257431\n",
      "[23]\tvalid_0's binary_logloss: 0.256882\n",
      "[24]\tvalid_0's binary_logloss: 0.256353\n",
      "[25]\tvalid_0's binary_logloss: 0.255865\n",
      "[26]\tvalid_0's binary_logloss: 0.255382\n",
      "[27]\tvalid_0's binary_logloss: 0.254983\n",
      "[28]\tvalid_0's binary_logloss: 0.254579\n",
      "[29]\tvalid_0's binary_logloss: 0.254161\n",
      "[30]\tvalid_0's binary_logloss: 0.25371\n",
      "[31]\tvalid_0's binary_logloss: 0.253374\n",
      "[32]\tvalid_0's binary_logloss: 0.253041\n",
      "[33]\tvalid_0's binary_logloss: 0.252738\n",
      "[34]\tvalid_0's binary_logloss: 0.252425\n",
      "[35]\tvalid_0's binary_logloss: 0.252077\n",
      "[36]\tvalid_0's binary_logloss: 0.251803\n",
      "[37]\tvalid_0's binary_logloss: 0.251561\n",
      "[38]\tvalid_0's binary_logloss: 0.251336\n",
      "[39]\tvalid_0's binary_logloss: 0.251104\n",
      "[40]\tvalid_0's binary_logloss: 0.250897\n",
      "[41]\tvalid_0's binary_logloss: 0.250647\n",
      "[42]\tvalid_0's binary_logloss: 0.250424\n",
      "[43]\tvalid_0's binary_logloss: 0.250167\n",
      "[44]\tvalid_0's binary_logloss: 0.249986\n",
      "[45]\tvalid_0's binary_logloss: 0.249753\n",
      "[46]\tvalid_0's binary_logloss: 0.249591\n",
      "[47]\tvalid_0's binary_logloss: 0.249449\n",
      "[48]\tvalid_0's binary_logloss: 0.249235\n",
      "[49]\tvalid_0's binary_logloss: 0.249088\n",
      "[50]\tvalid_0's binary_logloss: 0.248934\n",
      "[51]\tvalid_0's binary_logloss: 0.248767\n",
      "[52]\tvalid_0's binary_logloss: 0.248628\n",
      "[53]\tvalid_0's binary_logloss: 0.248495\n",
      "[54]\tvalid_0's binary_logloss: 0.248315\n",
      "[55]\tvalid_0's binary_logloss: 0.248212\n",
      "[56]\tvalid_0's binary_logloss: 0.248085\n",
      "[57]\tvalid_0's binary_logloss: 0.247965\n",
      "[58]\tvalid_0's binary_logloss: 0.247829\n",
      "[59]\tvalid_0's binary_logloss: 0.247706\n",
      "[60]\tvalid_0's binary_logloss: 0.247611\n",
      "[61]\tvalid_0's binary_logloss: 0.247548\n",
      "[62]\tvalid_0's binary_logloss: 0.247466\n",
      "[63]\tvalid_0's binary_logloss: 0.247367\n",
      "[64]\tvalid_0's binary_logloss: 0.247254\n",
      "[65]\tvalid_0's binary_logloss: 0.247145\n",
      "[66]\tvalid_0's binary_logloss: 0.247039\n",
      "[67]\tvalid_0's binary_logloss: 0.246907\n",
      "[68]\tvalid_0's binary_logloss: 0.246759\n",
      "[69]\tvalid_0's binary_logloss: 0.24669\n",
      "[70]\tvalid_0's binary_logloss: 0.24657\n",
      "[71]\tvalid_0's binary_logloss: 0.246525\n",
      "[72]\tvalid_0's binary_logloss: 0.246406\n",
      "[73]\tvalid_0's binary_logloss: 0.246327\n",
      "[74]\tvalid_0's binary_logloss: 0.246278\n",
      "[75]\tvalid_0's binary_logloss: 0.246222\n",
      "[76]\tvalid_0's binary_logloss: 0.246123\n",
      "[77]\tvalid_0's binary_logloss: 0.246075\n",
      "[78]\tvalid_0's binary_logloss: 0.246035\n",
      "[79]\tvalid_0's binary_logloss: 0.245988\n",
      "[80]\tvalid_0's binary_logloss: 0.24595\n",
      "[81]\tvalid_0's binary_logloss: 0.245939\n",
      "[82]\tvalid_0's binary_logloss: 0.245875\n",
      "[83]\tvalid_0's binary_logloss: 0.245831\n",
      "[84]\tvalid_0's binary_logloss: 0.245814\n",
      "[85]\tvalid_0's binary_logloss: 0.245766\n",
      "[86]\tvalid_0's binary_logloss: 0.245742\n",
      "[87]\tvalid_0's binary_logloss: 0.24569\n",
      "[88]\tvalid_0's binary_logloss: 0.245654\n",
      "[89]\tvalid_0's binary_logloss: 0.245624\n",
      "[90]\tvalid_0's binary_logloss: 0.245571\n",
      "[91]\tvalid_0's binary_logloss: 0.245526\n",
      "[92]\tvalid_0's binary_logloss: 0.245501\n",
      "[93]\tvalid_0's binary_logloss: 0.245466\n",
      "[94]\tvalid_0's binary_logloss: 0.245429\n",
      "[95]\tvalid_0's binary_logloss: 0.245397\n",
      "[96]\tvalid_0's binary_logloss: 0.245385\n",
      "[97]\tvalid_0's binary_logloss: 0.24535\n",
      "[98]\tvalid_0's binary_logloss: 0.245272\n",
      "[99]\tvalid_0's binary_logloss: 0.245239\n",
      "[100]\tvalid_0's binary_logloss: 0.245184\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.245184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.306061\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.297495\n",
      "[3]\tvalid_0's binary_logloss: 0.291089\n",
      "[4]\tvalid_0's binary_logloss: 0.286139\n",
      "[5]\tvalid_0's binary_logloss: 0.282086\n",
      "[6]\tvalid_0's binary_logloss: 0.278748\n",
      "[7]\tvalid_0's binary_logloss: 0.275869\n",
      "[8]\tvalid_0's binary_logloss: 0.273362\n",
      "[9]\tvalid_0's binary_logloss: 0.271249\n",
      "[10]\tvalid_0's binary_logloss: 0.269472\n",
      "[11]\tvalid_0's binary_logloss: 0.267852\n",
      "[12]\tvalid_0's binary_logloss: 0.266358\n",
      "[13]\tvalid_0's binary_logloss: 0.265152\n",
      "[14]\tvalid_0's binary_logloss: 0.264034\n",
      "[15]\tvalid_0's binary_logloss: 0.263108\n",
      "[16]\tvalid_0's binary_logloss: 0.262195\n",
      "[17]\tvalid_0's binary_logloss: 0.261367\n",
      "[18]\tvalid_0's binary_logloss: 0.260537\n",
      "[19]\tvalid_0's binary_logloss: 0.259902\n",
      "[20]\tvalid_0's binary_logloss: 0.259236\n",
      "[21]\tvalid_0's binary_logloss: 0.258643\n",
      "[22]\tvalid_0's binary_logloss: 0.258055\n",
      "[23]\tvalid_0's binary_logloss: 0.257518\n",
      "[24]\tvalid_0's binary_logloss: 0.256984\n",
      "[25]\tvalid_0's binary_logloss: 0.256526\n",
      "[26]\tvalid_0's binary_logloss: 0.256073\n",
      "[27]\tvalid_0's binary_logloss: 0.255711\n",
      "[28]\tvalid_0's binary_logloss: 0.255274\n",
      "[29]\tvalid_0's binary_logloss: 0.254872\n",
      "[30]\tvalid_0's binary_logloss: 0.25453\n",
      "[31]\tvalid_0's binary_logloss: 0.254267\n",
      "[32]\tvalid_0's binary_logloss: 0.253934\n",
      "[33]\tvalid_0's binary_logloss: 0.253628\n",
      "[34]\tvalid_0's binary_logloss: 0.253349\n",
      "[35]\tvalid_0's binary_logloss: 0.253076\n",
      "[36]\tvalid_0's binary_logloss: 0.252819\n",
      "[37]\tvalid_0's binary_logloss: 0.252609\n",
      "[38]\tvalid_0's binary_logloss: 0.252384\n",
      "[39]\tvalid_0's binary_logloss: 0.252142\n",
      "[40]\tvalid_0's binary_logloss: 0.251981\n",
      "[41]\tvalid_0's binary_logloss: 0.251789\n",
      "[42]\tvalid_0's binary_logloss: 0.251605\n",
      "[43]\tvalid_0's binary_logloss: 0.251286\n",
      "[44]\tvalid_0's binary_logloss: 0.251102\n",
      "[45]\tvalid_0's binary_logloss: 0.250963\n",
      "[46]\tvalid_0's binary_logloss: 0.250798\n",
      "[47]\tvalid_0's binary_logloss: 0.250652\n",
      "[48]\tvalid_0's binary_logloss: 0.250501\n",
      "[49]\tvalid_0's binary_logloss: 0.250358\n",
      "[50]\tvalid_0's binary_logloss: 0.250133\n",
      "[51]\tvalid_0's binary_logloss: 0.24998\n",
      "[52]\tvalid_0's binary_logloss: 0.249812\n",
      "[53]\tvalid_0's binary_logloss: 0.249638\n",
      "[54]\tvalid_0's binary_logloss: 0.249543\n",
      "[55]\tvalid_0's binary_logloss: 0.249451\n",
      "[56]\tvalid_0's binary_logloss: 0.249348\n",
      "[57]\tvalid_0's binary_logloss: 0.249271\n",
      "[58]\tvalid_0's binary_logloss: 0.249151\n",
      "[59]\tvalid_0's binary_logloss: 0.24906\n",
      "[60]\tvalid_0's binary_logloss: 0.248953\n",
      "[61]\tvalid_0's binary_logloss: 0.248879\n",
      "[62]\tvalid_0's binary_logloss: 0.248805\n",
      "[63]\tvalid_0's binary_logloss: 0.248749\n",
      "[64]\tvalid_0's binary_logloss: 0.248616\n",
      "[65]\tvalid_0's binary_logloss: 0.248551\n",
      "[66]\tvalid_0's binary_logloss: 0.248504\n",
      "[67]\tvalid_0's binary_logloss: 0.248453\n",
      "[68]\tvalid_0's binary_logloss: 0.248346\n",
      "[69]\tvalid_0's binary_logloss: 0.248239\n",
      "[70]\tvalid_0's binary_logloss: 0.248185\n",
      "[71]\tvalid_0's binary_logloss: 0.248158\n",
      "[72]\tvalid_0's binary_logloss: 0.248036\n",
      "[73]\tvalid_0's binary_logloss: 0.247978\n",
      "[74]\tvalid_0's binary_logloss: 0.247934\n",
      "[75]\tvalid_0's binary_logloss: 0.247865\n",
      "[76]\tvalid_0's binary_logloss: 0.247763\n",
      "[77]\tvalid_0's binary_logloss: 0.247699\n",
      "[78]\tvalid_0's binary_logloss: 0.247648\n",
      "[79]\tvalid_0's binary_logloss: 0.247592\n",
      "[80]\tvalid_0's binary_logloss: 0.247538\n",
      "[81]\tvalid_0's binary_logloss: 0.247468\n",
      "[82]\tvalid_0's binary_logloss: 0.247423\n",
      "[83]\tvalid_0's binary_logloss: 0.247402\n",
      "[84]\tvalid_0's binary_logloss: 0.247332\n",
      "[85]\tvalid_0's binary_logloss: 0.24724\n",
      "[86]\tvalid_0's binary_logloss: 0.247163\n",
      "[87]\tvalid_0's binary_logloss: 0.247146\n",
      "[88]\tvalid_0's binary_logloss: 0.247093\n",
      "[89]\tvalid_0's binary_logloss: 0.246998\n",
      "[90]\tvalid_0's binary_logloss: 0.246948\n",
      "[91]\tvalid_0's binary_logloss: 0.246923\n",
      "[92]\tvalid_0's binary_logloss: 0.246876\n",
      "[93]\tvalid_0's binary_logloss: 0.246865\n",
      "[94]\tvalid_0's binary_logloss: 0.246859\n",
      "[95]\tvalid_0's binary_logloss: 0.246803\n",
      "[96]\tvalid_0's binary_logloss: 0.24678\n",
      "[97]\tvalid_0's binary_logloss: 0.246715\n",
      "[98]\tvalid_0's binary_logloss: 0.246671\n",
      "[99]\tvalid_0's binary_logloss: 0.246657\n",
      "[100]\tvalid_0's binary_logloss: 0.246623\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.246623\n",
      "[1]\tvalid_0's binary_logloss: 0.305762\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.296918\n",
      "[3]\tvalid_0's binary_logloss: 0.290551\n",
      "[4]\tvalid_0's binary_logloss: 0.285366\n",
      "[5]\tvalid_0's binary_logloss: 0.281124\n",
      "[6]\tvalid_0's binary_logloss: 0.277619\n",
      "[7]\tvalid_0's binary_logloss: 0.274606\n",
      "[8]\tvalid_0's binary_logloss: 0.272032\n",
      "[9]\tvalid_0's binary_logloss: 0.269808\n",
      "[10]\tvalid_0's binary_logloss: 0.267853\n",
      "[11]\tvalid_0's binary_logloss: 0.266041\n",
      "[12]\tvalid_0's binary_logloss: 0.264573\n",
      "[13]\tvalid_0's binary_logloss: 0.263307\n",
      "[14]\tvalid_0's binary_logloss: 0.262093\n",
      "[15]\tvalid_0's binary_logloss: 0.26105\n",
      "[16]\tvalid_0's binary_logloss: 0.260138\n",
      "[17]\tvalid_0's binary_logloss: 0.259289\n",
      "[18]\tvalid_0's binary_logloss: 0.258528\n",
      "[19]\tvalid_0's binary_logloss: 0.257774\n",
      "[20]\tvalid_0's binary_logloss: 0.257091\n",
      "[21]\tvalid_0's binary_logloss: 0.256513\n",
      "[22]\tvalid_0's binary_logloss: 0.255993\n",
      "[23]\tvalid_0's binary_logloss: 0.255435\n",
      "[24]\tvalid_0's binary_logloss: 0.254941\n",
      "[25]\tvalid_0's binary_logloss: 0.254399\n",
      "[26]\tvalid_0's binary_logloss: 0.253953\n",
      "[27]\tvalid_0's binary_logloss: 0.253501\n",
      "[28]\tvalid_0's binary_logloss: 0.253135\n",
      "[29]\tvalid_0's binary_logloss: 0.252761\n",
      "[30]\tvalid_0's binary_logloss: 0.252416\n",
      "[31]\tvalid_0's binary_logloss: 0.252013\n",
      "[32]\tvalid_0's binary_logloss: 0.251664\n",
      "[33]\tvalid_0's binary_logloss: 0.25135\n",
      "[34]\tvalid_0's binary_logloss: 0.251095\n",
      "[35]\tvalid_0's binary_logloss: 0.250855\n",
      "[36]\tvalid_0's binary_logloss: 0.250575\n",
      "[37]\tvalid_0's binary_logloss: 0.250324\n",
      "[38]\tvalid_0's binary_logloss: 0.250053\n",
      "[39]\tvalid_0's binary_logloss: 0.249877\n",
      "[40]\tvalid_0's binary_logloss: 0.249621\n",
      "[41]\tvalid_0's binary_logloss: 0.249238\n",
      "[42]\tvalid_0's binary_logloss: 0.249083\n",
      "[43]\tvalid_0's binary_logloss: 0.248889\n",
      "[44]\tvalid_0's binary_logloss: 0.248704\n",
      "[45]\tvalid_0's binary_logloss: 0.248501\n",
      "[46]\tvalid_0's binary_logloss: 0.248322\n",
      "[47]\tvalid_0's binary_logloss: 0.248157\n",
      "[48]\tvalid_0's binary_logloss: 0.247965\n",
      "[49]\tvalid_0's binary_logloss: 0.247747\n",
      "[50]\tvalid_0's binary_logloss: 0.247642\n",
      "[51]\tvalid_0's binary_logloss: 0.247505\n",
      "[52]\tvalid_0's binary_logloss: 0.24728\n",
      "[53]\tvalid_0's binary_logloss: 0.247166\n",
      "[54]\tvalid_0's binary_logloss: 0.24706\n",
      "[55]\tvalid_0's binary_logloss: 0.24687\n",
      "[56]\tvalid_0's binary_logloss: 0.246783\n",
      "[57]\tvalid_0's binary_logloss: 0.246615\n",
      "[58]\tvalid_0's binary_logloss: 0.246537\n",
      "[59]\tvalid_0's binary_logloss: 0.246374\n",
      "[60]\tvalid_0's binary_logloss: 0.246258\n",
      "[61]\tvalid_0's binary_logloss: 0.246093\n",
      "[62]\tvalid_0's binary_logloss: 0.246038\n",
      "[63]\tvalid_0's binary_logloss: 0.245959\n",
      "[64]\tvalid_0's binary_logloss: 0.245803\n",
      "[65]\tvalid_0's binary_logloss: 0.245672\n",
      "[66]\tvalid_0's binary_logloss: 0.245587\n",
      "[67]\tvalid_0's binary_logloss: 0.245511\n",
      "[68]\tvalid_0's binary_logloss: 0.245438\n",
      "[69]\tvalid_0's binary_logloss: 0.245368\n",
      "[70]\tvalid_0's binary_logloss: 0.245311\n",
      "[71]\tvalid_0's binary_logloss: 0.245258\n",
      "[72]\tvalid_0's binary_logloss: 0.245185\n",
      "[73]\tvalid_0's binary_logloss: 0.245122\n",
      "[74]\tvalid_0's binary_logloss: 0.245044\n",
      "[75]\tvalid_0's binary_logloss: 0.244969\n",
      "[76]\tvalid_0's binary_logloss: 0.244857\n",
      "[77]\tvalid_0's binary_logloss: 0.244812\n",
      "[78]\tvalid_0's binary_logloss: 0.244691\n",
      "[79]\tvalid_0's binary_logloss: 0.244649\n",
      "[80]\tvalid_0's binary_logloss: 0.24462\n",
      "[81]\tvalid_0's binary_logloss: 0.244571\n",
      "[82]\tvalid_0's binary_logloss: 0.244499\n",
      "[83]\tvalid_0's binary_logloss: 0.244492\n",
      "[84]\tvalid_0's binary_logloss: 0.244446\n",
      "[85]\tvalid_0's binary_logloss: 0.244338\n",
      "[86]\tvalid_0's binary_logloss: 0.244263\n",
      "[87]\tvalid_0's binary_logloss: 0.244252\n",
      "[88]\tvalid_0's binary_logloss: 0.244182\n",
      "[89]\tvalid_0's binary_logloss: 0.244122\n",
      "[90]\tvalid_0's binary_logloss: 0.244118\n",
      "[91]\tvalid_0's binary_logloss: 0.244082\n",
      "[92]\tvalid_0's binary_logloss: 0.244084\n",
      "[93]\tvalid_0's binary_logloss: 0.244006\n",
      "[94]\tvalid_0's binary_logloss: 0.243993\n",
      "[95]\tvalid_0's binary_logloss: 0.243979\n",
      "[96]\tvalid_0's binary_logloss: 0.243928\n",
      "[97]\tvalid_0's binary_logloss: 0.243897\n",
      "[98]\tvalid_0's binary_logloss: 0.243877\n",
      "[99]\tvalid_0's binary_logloss: 0.243841\n",
      "[100]\tvalid_0's binary_logloss: 0.243792\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.243792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.306178\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.297502\n",
      "[3]\tvalid_0's binary_logloss: 0.29099\n",
      "[4]\tvalid_0's binary_logloss: 0.285978\n",
      "[5]\tvalid_0's binary_logloss: 0.28181\n",
      "[6]\tvalid_0's binary_logloss: 0.278266\n",
      "[7]\tvalid_0's binary_logloss: 0.275277\n",
      "[8]\tvalid_0's binary_logloss: 0.272748\n",
      "[9]\tvalid_0's binary_logloss: 0.270449\n",
      "[10]\tvalid_0's binary_logloss: 0.268485\n",
      "[11]\tvalid_0's binary_logloss: 0.266836\n",
      "[12]\tvalid_0's binary_logloss: 0.26529\n",
      "[13]\tvalid_0's binary_logloss: 0.264022\n",
      "[14]\tvalid_0's binary_logloss: 0.262823\n",
      "[15]\tvalid_0's binary_logloss: 0.26175\n",
      "[16]\tvalid_0's binary_logloss: 0.260812\n",
      "[17]\tvalid_0's binary_logloss: 0.260009\n",
      "[18]\tvalid_0's binary_logloss: 0.259154\n",
      "[19]\tvalid_0's binary_logloss: 0.258341\n",
      "[20]\tvalid_0's binary_logloss: 0.257713\n",
      "[21]\tvalid_0's binary_logloss: 0.256994\n",
      "[22]\tvalid_0's binary_logloss: 0.256382\n",
      "[23]\tvalid_0's binary_logloss: 0.255852\n",
      "[24]\tvalid_0's binary_logloss: 0.255334\n",
      "[25]\tvalid_0's binary_logloss: 0.254861\n",
      "[26]\tvalid_0's binary_logloss: 0.254439\n",
      "[27]\tvalid_0's binary_logloss: 0.253993\n",
      "[28]\tvalid_0's binary_logloss: 0.25359\n",
      "[29]\tvalid_0's binary_logloss: 0.253213\n",
      "[30]\tvalid_0's binary_logloss: 0.252848\n",
      "[31]\tvalid_0's binary_logloss: 0.252514\n",
      "[32]\tvalid_0's binary_logloss: 0.252202\n",
      "[33]\tvalid_0's binary_logloss: 0.251869\n",
      "[34]\tvalid_0's binary_logloss: 0.251551\n",
      "[35]\tvalid_0's binary_logloss: 0.251256\n",
      "[36]\tvalid_0's binary_logloss: 0.251\n",
      "[37]\tvalid_0's binary_logloss: 0.250723\n",
      "[38]\tvalid_0's binary_logloss: 0.250486\n",
      "[39]\tvalid_0's binary_logloss: 0.250235\n",
      "[40]\tvalid_0's binary_logloss: 0.250008\n",
      "[41]\tvalid_0's binary_logloss: 0.249726\n",
      "[42]\tvalid_0's binary_logloss: 0.249539\n",
      "[43]\tvalid_0's binary_logloss: 0.249313\n",
      "[44]\tvalid_0's binary_logloss: 0.249144\n",
      "[45]\tvalid_0's binary_logloss: 0.248955\n",
      "[46]\tvalid_0's binary_logloss: 0.248733\n",
      "[47]\tvalid_0's binary_logloss: 0.248478\n",
      "[48]\tvalid_0's binary_logloss: 0.248276\n",
      "[49]\tvalid_0's binary_logloss: 0.24815\n",
      "[50]\tvalid_0's binary_logloss: 0.248021\n",
      "[51]\tvalid_0's binary_logloss: 0.247863\n",
      "[52]\tvalid_0's binary_logloss: 0.247691\n",
      "[53]\tvalid_0's binary_logloss: 0.247483\n",
      "[54]\tvalid_0's binary_logloss: 0.24735\n",
      "[55]\tvalid_0's binary_logloss: 0.24717\n",
      "[56]\tvalid_0's binary_logloss: 0.247046\n",
      "[57]\tvalid_0's binary_logloss: 0.246951\n",
      "[58]\tvalid_0's binary_logloss: 0.246829\n",
      "[59]\tvalid_0's binary_logloss: 0.24674\n",
      "[60]\tvalid_0's binary_logloss: 0.246608\n",
      "[61]\tvalid_0's binary_logloss: 0.246499\n",
      "[62]\tvalid_0's binary_logloss: 0.246434\n",
      "[63]\tvalid_0's binary_logloss: 0.246284\n",
      "[64]\tvalid_0's binary_logloss: 0.246116\n",
      "[65]\tvalid_0's binary_logloss: 0.246046\n",
      "[66]\tvalid_0's binary_logloss: 0.245978\n",
      "[67]\tvalid_0's binary_logloss: 0.245842\n",
      "[68]\tvalid_0's binary_logloss: 0.245723\n",
      "[69]\tvalid_0's binary_logloss: 0.245602\n",
      "[70]\tvalid_0's binary_logloss: 0.245489\n",
      "[71]\tvalid_0's binary_logloss: 0.245366\n",
      "[72]\tvalid_0's binary_logloss: 0.24527\n",
      "[73]\tvalid_0's binary_logloss: 0.245202\n",
      "[74]\tvalid_0's binary_logloss: 0.245172\n",
      "[75]\tvalid_0's binary_logloss: 0.245082\n",
      "[76]\tvalid_0's binary_logloss: 0.245031\n",
      "[77]\tvalid_0's binary_logloss: 0.244938\n",
      "[78]\tvalid_0's binary_logloss: 0.244904\n",
      "[79]\tvalid_0's binary_logloss: 0.244839\n",
      "[80]\tvalid_0's binary_logloss: 0.244748\n",
      "[81]\tvalid_0's binary_logloss: 0.244712\n",
      "[82]\tvalid_0's binary_logloss: 0.244646\n",
      "[83]\tvalid_0's binary_logloss: 0.24458\n",
      "[84]\tvalid_0's binary_logloss: 0.24455\n",
      "[85]\tvalid_0's binary_logloss: 0.244497\n",
      "[86]\tvalid_0's binary_logloss: 0.244461\n",
      "[87]\tvalid_0's binary_logloss: 0.244428\n",
      "[88]\tvalid_0's binary_logloss: 0.244394\n",
      "[89]\tvalid_0's binary_logloss: 0.244337\n",
      "[90]\tvalid_0's binary_logloss: 0.244285\n",
      "[91]\tvalid_0's binary_logloss: 0.244254\n",
      "[92]\tvalid_0's binary_logloss: 0.244222\n",
      "[93]\tvalid_0's binary_logloss: 0.244194\n",
      "[94]\tvalid_0's binary_logloss: 0.2441\n",
      "[95]\tvalid_0's binary_logloss: 0.24406\n",
      "[96]\tvalid_0's binary_logloss: 0.24404\n",
      "[97]\tvalid_0's binary_logloss: 0.244018\n",
      "[98]\tvalid_0's binary_logloss: 0.244005\n",
      "[99]\tvalid_0's binary_logloss: 0.243998\n",
      "[100]\tvalid_0's binary_logloss: 0.243995\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.243995\n",
      "LGBM modelpair6 finished, time spent 403.71seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_clf0_0 = lgb.LGBMClassifier()\n",
    "lgbm_recurrent_month['pair6'] = LGBM(lgbm_clf0_0,X_GB,Y_GB,big_sheet_N,'pair6',if_month=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7821926189655073"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_recurrent_month['pair6'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end_nodes = outside_data_tree3.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "\n",
    "NR_nodes = outside_data_tree3.NR_nodes\n",
    "\n",
    "lgbm3_return_dict={}\n",
    "\n",
    "for i in nodes_id:\n",
    "    \n",
    "    X_GB = big_sheet_GB.iloc[end_nodes[i],:].drop(['id','bad'],axis=1)\n",
    "    Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0}) \n",
    "    X_N = big_sheet_N.iloc[NR_nodes[i]]\n",
    "    \n",
    "    lgbm3_return_dict[i] = LGBM(lgbm_clf0_1,X_GB,Y_GB,X_N,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_nodes = outside_data_tree5.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "\n",
    "NR_nodes = outside_data_tree5.NR_nodes\n",
    "\n",
    "list1 = [27,25,34,35]\n",
    "list2 = [28,24,31,7]\n",
    "list3 = [11,16,32,40,15]\n",
    "list4 = [45,48,5,44,4]\n",
    "list5 = [38,8,41,12,18,47,19]\n",
    "\n",
    "lgbm5_return_dict={}\n",
    "for i in nodes_id:\n",
    "    \n",
    "    X_GB = big_sheet_GB.iloc[end_nodes[i],:].drop(['id','bad'],axis=1)\n",
    "    Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0}) \n",
    "    X_NR = big_sheet_N.iloc[NR_nodes[i]]\n",
    "    \n",
    "    if i in list1:\n",
    "        lgbm5_return_dict[i] = LGBM(lgbm_clf1,X_GB,Y_GB,X_NR,i)\n",
    "        \n",
    "    elif i in list2:\n",
    "        lgbm5_return_dict[i] = LGBM(lgbm_clf2,X_GB,Y_GB,X_NR,i)\n",
    "        \n",
    "    elif i in list3:\n",
    "        lgbm5_return_dict[i] = LGBM(lgbm_clf4,X_GB,Y_GB,X_NR,i)\n",
    "        \n",
    "    elif i in list4:\n",
    "        lgbm5_return_dict[i] = LGBM(lgbm_clf4,X_GB,Y_GB,X_NR,i)\n",
    "    \n",
    "    else:\n",
    "        lgbm5_return_dict[i] = LGBM(lgbm_clf4,X_GB,Y_GB,X_NR,i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [auc,ks,Y_test,pred,auc_train,ks_train,\n",
    "# Y_train,pred_train,train_good,train_bad,test_good,test_bad,total_good,total_bad,\n",
    "# ftimp,NR_pred_df，GB_thres]\n",
    "\n",
    "# outside_data_tree\n",
    "def MakeLGBMResult(lgbm_return_dict):\n",
    "    \n",
    "    auc,auc_train,i_s,ks,ks_train,num_samples,GB_thres = [],[],[],[],[],[],[]\n",
    "    train_good,train_bad,test_good,test_bad,total_good,total_bad = [],[],[],[],[],[]\n",
    "\n",
    "    ftimps = {}\n",
    "#     ftimps['fts'] = mother_X.columns\n",
    "#     ftimps['mother'] = mother_lgbm_ftimp\n",
    "    \n",
    "    lgbm_preds_dfs = {}\n",
    "    \n",
    "    lgbm_clfs = {}\n",
    "\n",
    "    # 各子群结果\n",
    "    for i in lgbm_return_dict.keys():\n",
    "\n",
    "        i_s.append(i)\n",
    "        auc.append(lgbm_return_dict[i][0])\n",
    "        ks.append(lgbm_return_dict[i][1])\n",
    "        auc_train.append(lgbm_return_dict[i][4])\n",
    "        ks_train.append(lgbm_return_dict[i][5])\n",
    "#         num_samples.append(len(outside_data_tree.end_nodes[i]))\n",
    "        train_good.append(lgbm_return_dict[i][8])\n",
    "        train_bad.append(lgbm_return_dict[i][9])\n",
    "        test_good.append(lgbm_return_dict[i][10])\n",
    "        test_bad.append(lgbm_return_dict[i][11])\n",
    "        total_good.append(lgbm_return_dict[i][12])\n",
    "        total_bad.append(lgbm_return_dict[i][13])\n",
    "        GB_thres.append(lgbm_return_dict[i][-2])\n",
    "        ftimps[i] = lgbm_return_dict[i][14]\n",
    "        lgbm_preds_dfs[i] = lgbm_return_dict[i][-3]\n",
    "        lgbm_clfs[i] = lgbm_return_dict[i][-1]\n",
    "     \n",
    "    # append母群结果\n",
    "#     i_s.append('mother')\n",
    "#     auc.append(lgbm_mother_auc)\n",
    "#     ks.append(lgbm_mother_ks)\n",
    "#     auc_train.append(lgbm_mother_auc_train)\n",
    "#     ks_train.append(lgbm_mother_ks_train)\n",
    "#     num_samples.append(393294)\n",
    "#     train_good.append(lgbm_mother_train_good)\n",
    "#     train_bad.append(lgbm_mother_train_bad)\n",
    "#     test_good.append(lgbm_mother_test_good)\n",
    "#     test_bad.append(lgbm_mother_test_bad)\n",
    "#     total_good.append(lgbm_mother_total_good)\n",
    "#     total_bad.append(lgbm_mother_total_bad)\n",
    "#     GB_thres.append(lgbm_mother_GBthres)\n",
    "#     lgbm_preds_dfs['mother'] = lgbm_mother_NRpred\n",
    "    \n",
    "    y_test_stack = np.array([])\n",
    "    y_pred_stack = np.array([])\n",
    "    y_pred_train_stack = np.array([])\n",
    "    y_train_stack = np.array([])\n",
    "    \n",
    "    # 子群整合计算表现\n",
    "    for i in lgbm_return_dict.keys():\n",
    "\n",
    "        y_test_stack = np.concatenate([y_test_stack,lgbm_return_dict[i][2]])\n",
    "        y_pred_stack = np.concatenate([y_pred_stack,lgbm_return_dict[i][3]])\n",
    "        y_pred_train_stack = np.concatenate([y_pred_train_stack,lgbm_return_dict[i][7]])\n",
    "        y_train_stack = np.concatenate([y_train_stack,lgbm_return_dict[i][6]])            \n",
    "    \n",
    "#     total_auc = roc_auc_score(y_test_stack,y_pred_stack)\n",
    "#     fpr,tpr,thresholds= roc_curve(y_test_stack,y_pred_stack)\n",
    "#     total_ks = max(tpr-fpr)\n",
    "#     total_GBthres = 1 - thresholds[np.argmax(tpr-fpr)]\n",
    "\n",
    "#     total_auc_train = roc_auc_score(y_train_stack,y_pred_train_stack)\n",
    "#     fpr,tpr,thresholds= roc_curve(y_train_stack,y_pred_train_stack)\n",
    "#     total_ks_train = max(tpr-fpr)\n",
    "\n",
    "#     # append子群整合后的结果\n",
    "#     i_s.append('子群整合')\n",
    "#     auc.append(total_auc)\n",
    "#     ks.append(total_ks)\n",
    "#     auc_train.append(total_auc_train)\n",
    "#     ks_train.append(total_ks_train)\n",
    "#     num_samples.append(393294)\n",
    "#     train_good.append(lgbm_mother_train_good)\n",
    "#     train_bad.append(lgbm_mother_train_bad)\n",
    "#     test_good.append(lgbm_mother_test_good)\n",
    "#     test_bad.append(lgbm_mother_test_bad)\n",
    "#     total_good.append(lgbm_mother_total_good)\n",
    "#     total_bad.append(lgbm_mother_total_bad)\n",
    "#     GB_thres.append(total_GBthres)\n",
    "    \n",
    "    model_result_df = pd.DataFrame({'月份组合':i_s,'test_good':test_good,'test_bad':test_bad,'test_auc':auc,'test_ks':ks,\n",
    "                              'train_good':train_good,'train_bad':train_bad,'train_auc':auc_train,'train_ks':ks_train,\n",
    "                              'total_good':total_good,'total_bad':total_bad,'GB_thres':GB_thres}).sort_values(by=['test_ks','test_auc'],ascending=False)\n",
    "    # 'num_sample':num_samples,\n",
    "    model_result_df['train_test_gap'] = model_result_df['train_auc'] - model_result_df['test_auc']\n",
    "    feature_importance_df = pd.DataFrame(ftimps)\n",
    "    \n",
    "    return model_result_df,feature_importance_df,lgbm_preds_dfs,lgbm_clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_monthly_result,lgbm3_ftimp,lgbm_preds_df,lgbm_clfs5_6 = MakeLGBMResult(lgbm_recurrent_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_monthly_result['GB_thres'] = 1-lgbm_monthly_result['GB_thres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>月份组合</th>\n",
       "      <th>test_good</th>\n",
       "      <th>test_bad</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_ks</th>\n",
       "      <th>train_good</th>\n",
       "      <th>train_bad</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>train_ks</th>\n",
       "      <th>total_good</th>\n",
       "      <th>total_bad</th>\n",
       "      <th>GB_thres</th>\n",
       "      <th>train_test_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pair5</td>\n",
       "      <td>195157</td>\n",
       "      <td>17951</td>\n",
       "      <td>0.820796</td>\n",
       "      <td>0.488581</td>\n",
       "      <td>428193</td>\n",
       "      <td>50769</td>\n",
       "      <td>0.824326</td>\n",
       "      <td>0.490728</td>\n",
       "      <td>623350</td>\n",
       "      <td>68720</td>\n",
       "      <td>0.896617</td>\n",
       "      <td>0.003530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pair6</td>\n",
       "      <td>242944</td>\n",
       "      <td>29404</td>\n",
       "      <td>0.782193</td>\n",
       "      <td>0.419713</td>\n",
       "      <td>434781</td>\n",
       "      <td>47052</td>\n",
       "      <td>0.847045</td>\n",
       "      <td>0.528897</td>\n",
       "      <td>677725</td>\n",
       "      <td>76456</td>\n",
       "      <td>0.916463</td>\n",
       "      <td>0.064853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    月份组合  test_good  test_bad  test_auc   test_ks  train_good  train_bad  \\\n",
       "0  pair5     195157     17951  0.820796  0.488581      428193      50769   \n",
       "1  pair6     242944     29404  0.782193  0.419713      434781      47052   \n",
       "\n",
       "   train_auc  train_ks  total_good  total_bad  GB_thres  train_test_gap  \n",
       "0   0.824326  0.490728      623350      68720  0.896617        0.003530  \n",
       "1   0.847045  0.528897      677725      76456  0.916463        0.064853  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_monthly_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_monthly_result.to_csv('Monthly_rolling/pair5and6_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_preds_df['pair6'].to_csv('Monthly_rolling/pair6_predN.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X_GB.drop('month_mark',axis=1))\n",
    "pair1_predGB = lgbm_clfs5_6['pair6'].predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([big_sheet_GB[['id','month_mark']],\n",
    "           pd.DataFrame(pair1_predGB)],axis=1).to_csv('Monthly_rolling/pair6_predGB.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_preds_combined = pd.concat([lgbm_preds_df[4],lgbm_preds_df[5],lgbm_preds_df[7],lgbm_preds_df[8],\n",
    "                                 lgbm_preds_df[11],lgbm_preds_df[12],lgbm_preds_df[14],lgbm_preds_df[15],\n",
    "                                 lgbm_preds_df[19],lgbm_preds_df[20],lgbm_preds_df[22],lgbm_preds_df[23],\n",
    "                                 lgbm_preds_df[26],lgbm_preds_df[27],lgbm_preds_df[29],lgbm_preds_df[30]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_preds_combined = pd.concat([lgbm_preds_df[4],lgbm_preds_df[5],lgbm_preds_df[7],lgbm_preds_df[8],\n",
    "                                 lgbm_preds_df[11],lgbm_preds_df[12],lgbm_preds_df[15],lgbm_preds_df[16],\n",
    "                                 lgbm_preds_df[18],lgbm_preds_df[19],lgbm_preds_df[24],lgbm_preds_df[25],\n",
    "                                 lgbm_preds_df[27],lgbm_preds_df[28],lgbm_preds_df[31],lgbm_preds_df[32],\n",
    "                                 lgbm_preds_df[34],lgbm_preds_df[35],lgbm_preds_df[38],lgbm_preds_df[40],\n",
    "                                 lgbm_preds_df[41],lgbm_preds_df[44],lgbm_preds_df[45],lgbm_preds_df[47],\n",
    "                                 lgbm_preds_df[48]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_preds_combined = pd.concat([lgbm_preds_df[4],lgbm_preds_df[5],lgbm_preds_df[8],lgbm_preds_df[9],\n",
    "                                 lgbm_preds_df[11],lgbm_preds_df[12],lgbm_preds_df[17],lgbm_preds_df[19],\n",
    "                                 lgbm_preds_df[20],lgbm_preds_df[23],lgbm_preds_df[24],lgbm_preds_df[26],\n",
    "                                 lgbm_preds_df[27],lgbm_preds_df[32],lgbm_preds_df[33],lgbm_preds_df[35],\n",
    "                                 lgbm_preds_df[36],lgbm_preds_df[39],lgbm_preds_df[40],lgbm_preds_df[42],\n",
    "                                 lgbm_preds_df[43],lgbm_preds_df[46],lgbm_preds_df[48],lgbm_preds_df[49],\n",
    "                                 lgbm_preds_df[52],lgbm_preds_df[53],lgbm_preds_df[55],lgbm_preds_df[56]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lgbm_preds_combined['pred_good_prob']>=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_preds_combined.to_csv('lgbm25_NoR_pred_0427.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_preds_combined.to_csv('lgbm16_addR_pred_0427.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_preds_combined.to_csv('lgbm29_addR_pred_0427.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_preds_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lgbm_preds_combined['pred_good_prob']<0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_X = big_sheet_GB.drop(['id','nasrdw_recd_date','bad'],axis=1)\n",
    "mother_Y = big_sheet_GB['bad'].replace({'G':1,'B':0})\n",
    "mother_N = big_sheet_N\n",
    "[xgb_mother_clf,xgb_mother_auc,xgb_mother_ks,xgb_mother_y_test,xgb_mother_pred,\n",
    " xgb_mother_auc_train,xgb_mother_ks_train,xgb_mother_y_train,xgb_mother_pred_train,\n",
    " xgb_mother_train_good,xgb_mother_train_bad,xgb_mother_test_good,xgb_mother_test_bad,\n",
    " xgb_mother_total_good,xgb_mother_total_bad,mother_ftimps,xgb_mother_NRpred] = XGB(mother_X,mother_Y,mother_N,'mother',xgb_clf0)\n",
    "mother_Y.value_counts(),xgb_mother_auc,xgb_mother_ks,xgb_mother_auc_train,xgb_mother_ks_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_nodes = outside_data_tree3.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "\n",
    "list1 = [9,8]\n",
    "list2 = [11,12,5,4,2]\n",
    "\n",
    "xgb3_return_dict={}\n",
    "for i in nodes_id:\n",
    "    \n",
    "    X_GB = big_sheet_GB.iloc[end_nodes[i],:].drop(['id','bad'],axis=1)\n",
    "    Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0})  \n",
    "    \n",
    "    if i in list1:\n",
    "        xgb3_return_dict[i] = XGB(X_GB,Y_GB,i,xgb_clf1)\n",
    "    else:\n",
    "        xgb3_return_dict[i] = XGB(X_GB,Y_GB,i,xgb_clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3_result,xgb3_ftimp = MakeLGBMResult(xgb3_return_dict,outside_data_tree3)\n",
    "xgb3_result['train_test_gap'] = xgb3_result['train_auc']-xgb3_result['test_auc']\n",
    "xgb3_result.sort_values(by='train_test_gap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV+LR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_X = big_sheet_GB[mother_iv_fts]\n",
    "mother_Y = big_sheet_GB['bad'].replace({'G':1,'B':0})\n",
    "[mother_clf,mother_auc,mother_ks,mother_y_test,mother_pred,\n",
    " mother_auc_train,mother_ks_train,mother_y_train,mother_pred_train,\n",
    " mother_used_fts,\n",
    " mother_train_good,mother_train_bad,mother_test_good,mother_test_bad,mother_total_good,mother_total_bad,\n",
    " mother_coefs] = LR(mother_X,mother_Y,'mother',{})\n",
    "mother_Y.value_counts(),mother_auc,mother_ks,mother_auc_train,mother_ks_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "end_nodes_iv3 = outside_data_tree3.end_nodes\n",
    "end_nodes_iv_keys = list(end_nodes_iv3.keys())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    manager = mp.Manager()\n",
    "    iv_dict3 = manager.dict()\n",
    "    jobs = []\n",
    "    for i in end_nodes_iv_keys:\n",
    "        \n",
    "        X_GB = big_sheet_GB.iloc[end_nodes_iv3[i],:].drop(['id','bad'],axis=1)\n",
    "        Y_GB = big_sheet_GB['bad'].iloc[end_nodes_iv3[i]].replace({'G':1,'B':0})\n",
    "        IV_cols = list(X_GB.columns)\n",
    "        p = mp.Process(target=Run_IV, args=(X_GB,Y_GB,IV_cols,iv_dict3,i))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for proc in jobs:\n",
    "        proc.join()\n",
    "        \n",
    "end = time.time()\n",
    "print('whole IV procedure takes {}seconds'.format(np.round(end-start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "end_nodes_iv4 = outside_data_tree4.end_nodes\n",
    "end_nodes_iv_keys = list(end_nodes_iv4.keys())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    manager = mp.Manager()\n",
    "    iv_dict4 = manager.dict()\n",
    "    jobs = []\n",
    "    for i in end_nodes_iv_keys:\n",
    "        \n",
    "        X_GB = big_sheet_GB.iloc[end_nodes_iv4[i],:].drop(['id','bad'],axis=1)\n",
    "        Y_GB = big_sheet_GB['bad'].iloc[end_nodes_iv4[i]].replace({'G':1,'B':0})\n",
    "        IV_cols = list(X_GB.columns)\n",
    "        p = mp.Process(target=Run_IV, args=(X_GB,Y_GB,IV_cols,iv_dict4,i))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for proc in jobs:\n",
    "        proc.join()\n",
    "        \n",
    "end = time.time()\n",
    "print('whole IV procedure takes {}seconds'.format(np.round(end-start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "end_nodes_iv5 = outside_data_tree5.end_nodes\n",
    "end_nodes_iv_keys = list(end_nodes_iv5.keys())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    manager = mp.Manager()\n",
    "    iv_dict5 = manager.dict()\n",
    "    jobs = []\n",
    "    for i in end_nodes_iv_keys:\n",
    "        \n",
    "        X_GB = big_sheet_GB.iloc[end_nodes_iv5[i],:].drop(['id','bad'],axis=1)\n",
    "        Y_GB = big_sheet_GB['bad'].iloc[end_nodes_iv5[i]].replace({'G':1,'B':0})\n",
    "        IV_cols = list(X_GB.columns)\n",
    "        p = mp.Process(target=Run_IV, args=(X_GB,Y_GB,IV_cols,iv_dict5,i))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for proc in jobs:\n",
    "        proc.join()\n",
    "        \n",
    "end = time.time()\n",
    "print('whole IV procedure takes {}seconds'.format(np.round(end-start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_thres=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_iv_fts3 = {}\n",
    "for i in list(iv_dict3.keys()):\n",
    "    cond = iv_dict3[i]>iv_thres\n",
    "    children_iv_fts3[i] = list(iv_dict3[i].loc[cond].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_iv_fts4 = {}\n",
    "for i in list(iv_dict4.keys()):\n",
    "    cond = iv_dict4[i]>iv_thres\n",
    "    children_iv_fts4[i] = list(iv_dict4[i].loc[cond].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_iv_fts5 = {}\n",
    "for i in list(iv_dict5.keys()):\n",
    "    cond = iv_dict5[i]>iv_thres\n",
    "    children_iv_fts5[i] = list(iv_dict5[i].loc[cond].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "end_nodes = outside_data_tree3.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "if __name__ == '__main__':\n",
    "    manager = mp.Manager()\n",
    "    lr3_return_dict = manager.dict()\n",
    "    jobs = []\n",
    "    for i in nodes_id:\n",
    "        \n",
    "        X_GB = big_sheet_GB[children_iv_fts3[i]].iloc[end_nodes[i],:]\n",
    "        Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0})\n",
    "        \n",
    "        p = mp.Process(target=LR, args=(X_GB,Y_GB,i,lr3_return_dict))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for proc in jobs:\n",
    "        proc.join()\n",
    "        \n",
    "end = time.time()\n",
    "print('total time cost of duplicates processing takes {}seconds'.format(np.round(end-start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "end_nodes = outside_data_tree4.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "if __name__ == '__main__':\n",
    "    manager = mp.Manager()\n",
    "    lr4_return_dict = manager.dict()\n",
    "    jobs = []\n",
    "    for i in nodes_id:\n",
    "        \n",
    "        X_GB = big_sheet_GB[children_iv_fts4[i]].iloc[end_nodes[i],:]\n",
    "        Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0})\n",
    "        \n",
    "        p = mp.Process(target=LR, args=(X_GB,Y_GB,i,lr4_return_dict))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for proc in jobs:\n",
    "        proc.join()\n",
    "        \n",
    "end = time.time()\n",
    "print('total time cost of duplicates processing takes {}seconds'.format(np.round(end-start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "end_nodes = outside_data_tree5.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "if __name__ == '__main__':\n",
    "    manager = mp.Manager()\n",
    "    lr5_return_dict = manager.dict()\n",
    "    jobs = []\n",
    "    for i in nodes_id:\n",
    "        \n",
    "        X_GB = big_sheet_GB[children_iv_fts5[i]].iloc[end_nodes[i],:]\n",
    "        Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0})\n",
    "        \n",
    "        p = mp.Process(target=LR, args=(X_GB,Y_GB,i,lr5_return_dict))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for proc in jobs:\n",
    "        proc.join()\n",
    "        \n",
    "end = time.time()\n",
    "print('total time cost of duplicates processing takes {}seconds'.format(np.round(end-start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc,auc_train,i_s,ks,ks_train,num_samples = [],[],[],[],[],[]\n",
    "\n",
    "train_good,train_bad,test_good,test_bad,total_good,total_bad = [],[],[],[],[],[]\n",
    "\n",
    "used_fts = {}\n",
    "coefs = {}\n",
    "coefs['fts'] = mother_X.columns\n",
    "coefs['mother']=mother_coefs\n",
    "\n",
    "for i in lr3_return_dict.keys():\n",
    "    i_s.append(i)\n",
    "    auc.append(lr3_return_dict[i][1])\n",
    "#     cv_score.append(sum(model_result[i]['cv_score'])/len(model_result[i]['cv_score']))\n",
    "    ks.append(lr3_return_dict[i][2])\n",
    "    auc_train.append(lr3_return_dict[i][5])\n",
    "    ks_train.append(lr3_return_dict[i][6])\n",
    "    num_samples.append(len(outside_data_tree3.end_nodes[i]))\n",
    "    train_good.append(lr3_return_dict[i][10])\n",
    "    train_bad.append(lr3_return_dict[i][11])\n",
    "    test_good.append(lr3_return_dict[i][12])\n",
    "    test_bad.append(lr3_return_dict[i][13])\n",
    "    total_good.append(lr3_return_dict[i][14])\n",
    "    total_bad.append(lr3_return_dict[i][15])\n",
    "    used_fts[i] = lr3_return_dict[i][9]\n",
    "    coefs[i]=lr3_return_dict[i][16]\n",
    "    \n",
    "i_s.append('mother')\n",
    "auc.append(mother_auc)\n",
    "ks.append(mother_ks)\n",
    "auc_train.append(mother_auc_train)\n",
    "ks_train.append(mother_ks_train)\n",
    "num_samples.append(393294)\n",
    "train_good.append(mother_train_good)\n",
    "train_bad.append(mother_train_bad)\n",
    "test_good.append(mother_test_good)\n",
    "test_bad.append(mother_test_bad)\n",
    "total_good.append(mother_total_good)\n",
    "total_bad.append(mother_total_bad)\n",
    "\n",
    "y_test_stack = np.array([])\n",
    "y_pred_stack = np.array([])\n",
    "y_pred_train_stack = np.array([])\n",
    "y_train_stack = np.array([])\n",
    "for i in lr3_return_dict.keys():\n",
    "    y_test_stack = np.concatenate([y_test_stack,lr3_return_dict[i][3]])\n",
    "    y_pred_stack = np.concatenate([y_pred_stack,lr3_return_dict[i][4]])\n",
    "    y_pred_train_stack = np.concatenate([y_pred_train_stack,lr3_return_dict[i][8]])\n",
    "    y_train_stack = np.concatenate([y_train_stack,lr3_return_dict[i][7]])\n",
    "    \n",
    "total_auc = roc_auc_score(y_test_stack,y_pred_stack)\n",
    "fpr,tpr,thresholds= roc_curve(y_test_stack,y_pred_stack)\n",
    "total_ks = max(tpr-fpr)\n",
    "\n",
    "total_auc_train = roc_auc_score(y_train_stack,y_pred_train_stack)\n",
    "fpr,tpr,thresholds= roc_curve(y_train_stack,y_pred_train_stack)\n",
    "total_ks_train = max(tpr-fpr)\n",
    "\n",
    "i_s.append('子群整合')\n",
    "auc.append(total_auc)\n",
    "ks.append(total_ks)\n",
    "auc_train.append(total_auc_train)\n",
    "ks_train.append(total_ks_train)\n",
    "num_samples.append(393294)\n",
    "train_good.append(mother_train_good)\n",
    "train_bad.append(mother_train_bad)\n",
    "test_good.append(mother_test_good)\n",
    "test_bad.append(mother_test_bad)\n",
    "total_good.append(mother_total_good)\n",
    "total_bad.append(mother_total_bad)\n",
    "\n",
    "total_auc,total_ks,total_auc_train,total_ks_train\n",
    "lr3_480_0423_iv_result_df = pd.DataFrame({'子群ID':i_s,'test_good':test_good,'test_bad':test_bad,'test_auc':auc,'test_ks':ks,\n",
    "                              'train_good':train_good,'train_bad':train_bad,'auc_train':auc_train,'ks_train':ks_train,\n",
    "                              'total_good':total_good,'total_bad':total_bad,'num_sample':num_samples}).sort_values(by=['test_ks','test_auc'],ascending=False)\n",
    "\n",
    "lr3_480_0423_iv_result_df.to_csv('lr3_iv_result_df_480_0423.csv')\n",
    "\n",
    "keys = list(children_iv_fts3.keys())\n",
    "\n",
    "lr3_480_iv_weights1 = pd.DataFrame([mother_iv_fts,mother_coefs]).T\n",
    "lr3_480_iv_weights1.columns = ['mother_fts','mother']\n",
    "\n",
    "lr3_480_iv_weights2 = pd.DataFrame([mother_iv_fts,mother_coefs]).T\n",
    "lr3_480_iv_weights2.columns = ['mother_fts','mother']\n",
    "\n",
    "for i in keys:\n",
    "    df = pd.DataFrame([children_iv_fts3[i],coefs[i]]).T\n",
    "    df.columns = ['mother_fts',str(i)+'_weights']\n",
    "    lr3_480_iv_weights1 = pd.merge(left =lr3_480_iv_weights1,right = df,\n",
    "                                   on='mother_fts',\n",
    "                                   how='outer',copy=False)\n",
    "    \n",
    "    df.columns = [str(i),str(i)+'_weights']\n",
    "    lr3_480_iv_weights2 = pd.concat([lr3_480_iv_weights2,df],axis=1)\n",
    "\n",
    "lr3_480_iv_weights1.fillna(0,inplace=True)\n",
    "lr3_480_iv_weights1.to_csv('lr3_iv_480_weights_verion1.csv',index=False,encoding='GBK')\n",
    "lr3_480_iv_weights2.to_csv('lr3_iv_480_weights_verion2.csv',index=False,encoding='GBK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV+LGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xgb_clf,auc,ks,y_test,pred[:,1],auc_train,ks_train,y_train,\n",
    "# pred_train[:,1],used_fts,train_good,train_bad,test_good,test_bad,total_good,total_bad\n",
    "mother_X = big_sheet_GB[mother_iv_fts]\n",
    "mother_Y = big_sheet_GB['bad'].replace({'G':1,'B':0})\n",
    "[lgbm_mother_clf,lgbm_mother_auc,lgbm_mother_ks,lgbm_mother_y_test,lgbm_mother_pred,\n",
    " lgbm_mother_auc_train,lgbm_mother_ks_train,lgbm_mother_y_train,lgbm_mother_pred_train,\n",
    " lgbm_mother_train_good,lgbm_mother_train_bad,lgbm_mother_test_good,lgbm_mother_test_bad,\n",
    " lgbm_mother_total_good,lgbm_mother_total_bad,mother_lgbm_ftimp] = LGBM(mother_X,mother_Y,'mother')\n",
    "mother_Y.value_counts(),lgbm_mother_auc,lgbm_mother_ks,lgbm_mother_auc_train,lgbm_mother_ks_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end_nodes = outside_data_tree3.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "\n",
    "lgbm3_return_dict={}\n",
    "for i in nodes_id:\n",
    "    \n",
    "    X_GB = big_sheet_GB[children_iv_fts3[i]].iloc[end_nodes[i],:]\n",
    "    Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0}) \n",
    "    lgbm3_return_dict[i] = LGBM(X_GB,Y_GB,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_nodes = outside_data_tree4.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "\n",
    "lgbm4_return_dict={}\n",
    "for i in nodes_id:\n",
    "    \n",
    "    X_GB = big_sheet_GB[children_iv_fts4[i]].iloc[end_nodes[i],:]\n",
    "    Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0}) \n",
    "    lgbm4_return_dict[i] = LGBM(X_GB,Y_GB,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_nodes = outside_data_tree5.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "\n",
    "lgbm5_return_dict={}\n",
    "for i in nodes_id:\n",
    "    \n",
    "    X_GB = big_sheet_GB[children_iv_fts5[i]].iloc[end_nodes[i],:]\n",
    "    Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0}) \n",
    "    lgbm5_return_dict[i] = LGBM(X_GB,Y_GB,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc,auc_train,i_s,ks,ks_train,num_samples = [],[],[],[],[],[]\n",
    "\n",
    "train_good,train_bad,test_good,test_bad,total_good,total_bad = [],[],[],[],[],[]\n",
    "\n",
    "ftimp = {}\n",
    "ftimp['mother'] = mother_lgbm_ftimp\n",
    "\n",
    "for i in lgbm5_return_dict.keys():\n",
    "    i_s.append(i)\n",
    "    auc.append(lgbm5_return_dict[i][1])\n",
    "#     cv_score.append(sum(model_result[i]['cv_score'])/len(model_result[i]['cv_score']))\n",
    "    ks.append(lgbm5_return_dict[i][2])\n",
    "    auc_train.append(lgbm5_return_dict[i][5])\n",
    "    ks_train.append(lgbm5_return_dict[i][6])\n",
    "    num_samples.append(len(outside_data_tree5.end_nodes[i]))\n",
    "    train_good.append(lgbm5_return_dict[i][9])\n",
    "    train_bad.append(lgbm5_return_dict[i][10])\n",
    "    test_good.append(lgbm5_return_dict[i][11])\n",
    "    test_bad.append(lgbm5_return_dict[i][12])\n",
    "    total_good.append(lgbm5_return_dict[i][13])\n",
    "    total_bad.append(lgbm5_return_dict[i][14])\n",
    "    ftimp[i] = lgbm5_return_dict[i][-1]\n",
    "    \n",
    "i_s.append('mother')\n",
    "auc.append(lgbm_mother_auc)\n",
    "ks.append(lgbm_mother_ks)\n",
    "auc_train.append(lgbm_mother_auc_train)\n",
    "ks_train.append(lgbm_mother_ks_train)\n",
    "num_samples.append(393294)\n",
    "train_good.append(lgbm_mother_train_good)\n",
    "train_bad.append(lgbm_mother_train_bad)\n",
    "test_good.append(lgbm_mother_test_good)\n",
    "test_bad.append(lgbm_mother_test_bad)\n",
    "total_good.append(lgbm_mother_total_good)\n",
    "total_bad.append(lgbm_mother_total_bad)\n",
    "\n",
    "y_test_stack = np.array([])\n",
    "y_pred_stack = np.array([])\n",
    "y_pred_train_stack = np.array([])\n",
    "y_train_stack = np.array([])\n",
    "for i in lgbm5_return_dict.keys():\n",
    "    y_test_stack = np.concatenate([y_test_stack,lgbm5_return_dict[i][3]])\n",
    "    y_pred_stack = np.concatenate([y_pred_stack,lgbm5_return_dict[i][4]])\n",
    "    y_pred_train_stack = np.concatenate([y_pred_train_stack,lgbm5_return_dict[i][8]])\n",
    "    y_train_stack = np.concatenate([y_train_stack,lgbm5_return_dict[i][7]])\n",
    "    \n",
    "total_auc = roc_auc_score(y_test_stack,y_pred_stack)\n",
    "fpr,tpr,thresholds= roc_curve(y_test_stack,y_pred_stack)\n",
    "total_ks = max(tpr-fpr)\n",
    "\n",
    "total_auc_train = roc_auc_score(y_train_stack,y_pred_train_stack)\n",
    "fpr,tpr,thresholds= roc_curve(y_train_stack,y_pred_train_stack)\n",
    "total_ks_train = max(tpr-fpr)\n",
    "\n",
    "i_s.append('子群整合')\n",
    "auc.append(total_auc)\n",
    "ks.append(total_ks)\n",
    "auc_train.append(total_auc_train)\n",
    "ks_train.append(total_ks_train)\n",
    "num_samples.append(393294)\n",
    "train_good.append(lgbm_mother_train_good)\n",
    "train_bad.append(lgbm_mother_train_bad)\n",
    "test_good.append(lgbm_mother_test_good)\n",
    "test_bad.append(lgbm_mother_test_bad)\n",
    "total_good.append(lgbm_mother_total_good)\n",
    "total_bad.append(lgbm_mother_total_bad)\n",
    "\n",
    "total_auc,total_ks,total_auc_train,total_ks_train\n",
    "lgbm5_iv_480_0423_result_df = pd.DataFrame({'子群ID':i_s,'test_good':test_good,'test_bad':test_bad,'test_auc':auc,'test_ks':ks,\n",
    "                              'train_good':train_good,'train_bad':train_bad,'train_auc':auc_train,'train_ks':ks_train,\n",
    "                              'total_good':total_good,'total_bad':total_bad,'num_sample':num_samples}).sort_values(by=['test_ks','test_auc'],ascending=False)\n",
    "\n",
    "lgbm5_iv_480_0423_result_df.to_csv('lgbm5_iv_result_df_480_0423.csv',index=False)\n",
    "\n",
    "keys = list(children_iv_fts5.keys())\n",
    "\n",
    "lgbm5_480_iv_weights1 = pd.DataFrame([mother_iv_fts,mother_lgbm_ftimp]).T\n",
    "lgbm5_480_iv_weights1.columns = ['mother_fts','mother_importances']\n",
    "\n",
    "lgbm5_480_iv_weights2 = pd.DataFrame([mother_iv_fts,mother_lgbm_ftimp]).T\n",
    "lgbm5_480_iv_weights2.columns = ['mother_fts','mother_importances']\n",
    "\n",
    "for i in keys:\n",
    "    df = pd.DataFrame([children_iv_fts5[i],ftimp[i]]).T\n",
    "    df.columns = ['mother_fts',str(i)+'_importances']\n",
    "    lgbm5_480_iv_weights1 = pd.merge(left =lgbm5_480_iv_weights1,right = df,\n",
    "                                   on='mother_fts',\n",
    "                                   how='outer',copy=False)\n",
    "    \n",
    "    df.columns = [str(i),str(i)+'_importances']\n",
    "    lgbm5_480_iv_weights2 = pd.concat([lgbm5_480_iv_weights2,df],axis=1)\n",
    "    \n",
    "lgbm5_480_iv_weights1.fillna(0,inplace=True)\n",
    "lgbm5_480_iv_weights1.to_csv('lgbm5_480_0423_iv_weights_version1.csv',index=False,encoding='GBK')\n",
    "lgbm5_480_iv_weights2.to_csv('lgbm5_480_0423_iv_weights_version2.csv',index=False,encoding='GBK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV+XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_X = big_sheet_GB[mother_iv_fts]\n",
    "mother_Y = big_sheet_GB['bad'].replace({'G':1,'B':0})\n",
    "[xgb_mother_clf,xgb_mother_auc,xgb_mother_ks,xgb_mother_y_test,xgb_mother_pred,\n",
    " xgb_mother_auc_train,xgb_mother_ks_train,xgb_mother_y_train,xgb_mother_pred_train,\n",
    " xgb_mother_train_good,xgb_mother_train_bad,xgb_mother_test_good,xgb_mother_test_bad,\n",
    " xgb_mother_total_good,xgb_mother_total_bad,mother_xgb_ftimps] = XGB(mother_X,mother_Y,'mother',params)\n",
    "mother_Y.value_counts(),xgb_mother_auc,xgb_mother_ks,xgb_mother_auc_train,xgb_mother_ks_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_nodes = outside_data_tree3.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "\n",
    "xgb3_return_dict={}\n",
    "for i in nodes_id:\n",
    "    \n",
    "    X_GB = big_sheet_GB[children_iv_fts3[i]].iloc[end_nodes[i],:]\n",
    "    Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0}) \n",
    "    xgb3_return_dict[i] = XGB(X_GB,Y_GB,i,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_nodes = outside_data_tree4.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "\n",
    "xgb4_return_dict={}\n",
    "for i in nodes_id:\n",
    "    \n",
    "    X_GB = big_sheet_GB[children_iv_fts4[i]].iloc[end_nodes[i],:]\n",
    "    Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0}) \n",
    "    xgb4_return_dict[i] = XGB(X_GB,Y_GB,i,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end_nodes = outside_data_tree5.end_nodes\n",
    "nodes_id = end_nodes.keys()\n",
    "\n",
    "xgb5_return_dict={}\n",
    "for i in nodes_id:\n",
    "    \n",
    "    X_GB = big_sheet_GB[children_iv_fts5[i]].iloc[end_nodes[i],:]\n",
    "    Y_GB = big_sheet_GB['bad'].iloc[end_nodes[i]].replace({'G':1,'B':0}) \n",
    "    xgb5_return_dict[i] = XGB(X_GB,Y_GB,i,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc,auc_train,i_s,ks,ks_train,num_samples = [],[],[],[],[],[]\n",
    "\n",
    "train_good,train_bad,test_good,test_bad,total_good,total_bad = [],[],[],[],[],[]\n",
    "\n",
    "ftimp = {}\n",
    "ftimp['mother'] = mother_xgb_ftimps\n",
    "\n",
    "for i in xgb5_return_dict.keys():\n",
    "    i_s.append(i)\n",
    "    auc.append(xgb5_return_dict[i][1])\n",
    "#     cv_score.append(sum(model_result[i]['cv_score'])/len(model_result[i]['cv_score']))\n",
    "    ks.append(xgb5_return_dict[i][2])\n",
    "    auc_train.append(xgb5_return_dict[i][5])\n",
    "    ks_train.append(xgb5_return_dict[i][6])\n",
    "    num_samples.append(len(outside_data_tree5.end_nodes[i]))\n",
    "    train_good.append(xgb5_return_dict[i][9])\n",
    "    train_bad.append(xgb5_return_dict[i][10])\n",
    "    test_good.append(xgb5_return_dict[i][11])\n",
    "    test_bad.append(xgb5_return_dict[i][12])\n",
    "    total_good.append(xgb5_return_dict[i][13])\n",
    "    total_bad.append(xgb5_return_dict[i][14])\n",
    "    ftimp[i] = xgb5_return_dict[i][-1]\n",
    "    \n",
    "i_s.append('mother')\n",
    "auc.append(xgb_mother_auc)\n",
    "ks.append(xgb_mother_ks)\n",
    "auc_train.append(xgb_mother_auc_train)\n",
    "ks_train.append(xgb_mother_ks_train)\n",
    "num_samples.append(393294)\n",
    "train_good.append(xgb_mother_train_good)\n",
    "train_bad.append(xgb_mother_train_bad)\n",
    "test_good.append(xgb_mother_test_good)\n",
    "test_bad.append(xgb_mother_test_bad)\n",
    "total_good.append(xgb_mother_total_good)\n",
    "total_bad.append(xgb_mother_total_bad)\n",
    "\n",
    "y_test_stack = np.array([])\n",
    "y_pred_stack = np.array([])\n",
    "y_pred_train_stack = np.array([])\n",
    "y_train_stack = np.array([])\n",
    "for i in xgb5_return_dict.keys():\n",
    "    y_test_stack = np.concatenate([y_test_stack,xgb5_return_dict[i][3]])\n",
    "    y_pred_stack = np.concatenate([y_pred_stack,xgb5_return_dict[i][4]])\n",
    "    y_pred_train_stack = np.concatenate([y_pred_train_stack,xgb5_return_dict[i][8]])\n",
    "    y_train_stack = np.concatenate([y_train_stack,xgb5_return_dict[i][7]])\n",
    "    \n",
    "total_auc = roc_auc_score(y_test_stack,y_pred_stack)\n",
    "fpr,tpr,thresholds= roc_curve(y_test_stack,y_pred_stack)\n",
    "total_ks = max(tpr-fpr)\n",
    "\n",
    "total_auc_train = roc_auc_score(y_train_stack,y_pred_train_stack)\n",
    "fpr,tpr,thresholds= roc_curve(y_train_stack,y_pred_train_stack)\n",
    "total_ks_train = max(tpr-fpr)\n",
    "\n",
    "i_s.append('子群整合')\n",
    "auc.append(total_auc)\n",
    "ks.append(total_ks)\n",
    "auc_train.append(total_auc_train)\n",
    "ks_train.append(total_ks_train)\n",
    "num_samples.append(393294)\n",
    "train_good.append(xgb_mother_train_good)\n",
    "train_bad.append(xgb_mother_train_bad)\n",
    "test_good.append(xgb_mother_test_good)\n",
    "test_bad.append(xgb_mother_test_bad)\n",
    "total_good.append(xgb_mother_total_good)\n",
    "total_bad.append(xgb_mother_total_bad)\n",
    "\n",
    "total_auc,total_ks,total_auc_train,total_ks_train\n",
    "xgb5_iv_480_0423_result_df = pd.DataFrame({'子群ID':i_s,'test_good':test_good,'test_bad':test_bad,'test_auc':auc,'test_ks':ks,\n",
    "                              'train_good':train_good,'train_bad':train_bad,'train_auc':auc_train,'train_ks':ks_train,\n",
    "                              'total_good':total_good,'total_bad':total_bad,'num_sample':num_samples}).sort_values(by=['test_ks','test_auc'],ascending=False)\n",
    "\n",
    "xgb5_iv_480_0423_result_df.to_csv('xgb5_iv_result_df_480_0423.csv',index=False)\n",
    "\n",
    "keys = list(children_iv_fts5.keys())\n",
    "\n",
    "xgb5_480_iv_weights1 = pd.DataFrame([mother_iv_fts,mother_xgb_ftimps]).T\n",
    "xgb5_480_iv_weights1.columns = ['mother_fts','mother_importances']\n",
    "\n",
    "xgb5_480_iv_weights2 = pd.DataFrame([mother_iv_fts,mother_xgb_ftimps]).T\n",
    "xgb5_480_iv_weights2.columns = ['mother_fts','mother_importances']\n",
    "\n",
    "for i in keys:\n",
    "    df = pd.DataFrame([children_iv_fts5[i],ftimp[i]]).T\n",
    "    df.columns = ['mother_fts',str(i)+'_importances']\n",
    "    xgb5_480_iv_weights1 = pd.merge(left =xgb5_480_iv_weights1,right = df,\n",
    "                                   on='mother_fts',\n",
    "                                   how='outer',copy=False)\n",
    "    \n",
    "    df.columns = [str(i),str(i)+'_importances']\n",
    "    xgb5_480_iv_weights2 = pd.concat([xgb5_480_iv_weights2,df],axis=1)\n",
    "    \n",
    "xgb5_480_iv_weights1.fillna(0,inplace=True)\n",
    "xgb5_480_iv_weights1.to_csv('xgb5_480_0423_iv_weights_version1.csv',index=False,encoding='GBK')\n",
    "xgb5_480_iv_weights2.to_csv('xgb5_480_0423_iv_weights_version2.csv',index=False,encoding='GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3_iv_480_0423_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "childrens_NR3 = outside_data_tree3.NR_nodes\n",
    "childrens_NR4 = outside_data_tree4.NR_nodes\n",
    "childrens_NR5 = outside_data_tree5.NR_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split8_NR_IDs = {}\n",
    "\n",
    "for i in childrens_NR3.keys():\n",
    "    split8_NR_IDs[i] = outside_data_tree3.df_NR.iloc[childrens_NR3[i]][['id','bad']]\n",
    "    \n",
    "split8_NR_X = {}\n",
    "\n",
    "for i in split8_NR_IDs.keys():\n",
    "    \n",
    "    df = pd.merge(left = big_sheet_NR.drop('bad',axis=1),\n",
    "                  right = split8_NR_IDs[i].drop('bad',axis=1),\n",
    "                  on='id',\n",
    "                  how='inner')\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(df.drop('id',axis=1))\n",
    "    split8_NR_X[i] = {}\n",
    "    split8_NR_X[i]['x'] = scaled\n",
    "    split8_NR_X[i]['df'] = df\n",
    "                                  \n",
    "    \n",
    "pred8_NR = {}\n",
    "\n",
    "for i in split8_NR_X.keys():\n",
    "    pred8_NR[i] = {}\n",
    "    pred8_NR[i]['id'] = split8_NR_X[i]['df']['id']\n",
    "    pred8_NR[i]['pred_prob']= lgbm3_return_dict[i][0].predict_proba(split8_NR_X[i]['x'])[:,1]\n",
    "    \n",
    "pred8_NR_df = {}\n",
    "\n",
    "for i in pred8_NR.keys():\n",
    "    \n",
    "    temp_df = pd.DataFrame(pred8_NR[i])\n",
    "    temp_df['split id'] = i\n",
    "    pred8_NR_df[i] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm3_return_dict[4][0].predict_proba(split8_NR_X[4]['x'])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm3_return_dict[3][0].predict_proba(split8_NR_X[4]['x'])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred8_NR_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.concat([pred8_NR_df[3],pred8_NR_df[4],pred8_NR_df[6],pred8_NR_df[7],\n",
    "                     pred8_NR_df[10],pred8_NR_df[11],pred8_NR_df[13],pred8_NR_df[14]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pred_df['pred_prob']<0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('lr8_pred_0426.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pred_df['pred_prob']<0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monthly Recurrent Estimation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 6.88 MB\n",
      "Memory usage after optimization is: 4.30 MB\n",
      "Decreased by 37.5%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 6.88 MB\n",
      "Memory usage after optimization is: 4.30 MB\n",
      "Decreased by 37.5%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 6.88 MB\n",
      "Memory usage after optimization is: 4.30 MB\n",
      "Decreased by 37.5%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 6.88 MB\n",
      "Memory usage after optimization is: 4.30 MB\n",
      "Decreased by 37.5%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 6.88 MB\n",
      "Memory usage after optimization is: 4.30 MB\n",
      "Decreased by 37.5%\n",
      " \n",
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 6.88 MB\n",
      "Memory usage after optimization is: 4.30 MB\n",
      "Decreased by 37.5%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "pair1_pred_N = pd.read_csv('Monthly_rolling/pair1_predN.csv').iloc[:,:-1]\n",
    "pair1_pred_N = mm.reduce_mem_usage(pair1_pred_N)\n",
    "pair1_pred_N.columns = ['id','pair1_predN']\n",
    "\n",
    "pair2_pred_N = pd.read_csv('Monthly_rolling/pair2_predN.csv').iloc[:,:-1]\n",
    "pair2_pred_N = mm.reduce_mem_usage(pair2_pred_N)\n",
    "pair2_pred_N.columns = ['id','pair2_predN']\n",
    "\n",
    "pair3_pred_N = pd.read_csv('Monthly_rolling/pair3_predN.csv').iloc[:,:-1]\n",
    "pair3_pred_N = mm.reduce_mem_usage(pair3_pred_N)\n",
    "pair3_pred_N.columns = ['id','pair3_predN']\n",
    "\n",
    "pair4_pred_N = pd.read_csv('Monthly_rolling/pair4_predN.csv').iloc[:,:-1]\n",
    "pair4_pred_N = mm.reduce_mem_usage(pair4_pred_N)\n",
    "pair4_pred_N.columns = ['id','pair4_predN']\n",
    "\n",
    "pair5_pred_N = pd.read_csv('Monthly_rolling/pair5_predN.csv').iloc[:,:-1]\n",
    "pair5_pred_N = mm.reduce_mem_usage(pair5_pred_N)\n",
    "pair5_pred_N.columns = ['id','pair5_predN']\n",
    "\n",
    "pair6_pred_N = pd.read_csv('Monthly_rolling/pair6_predN.csv').iloc[:,:-1]\n",
    "pair6_pred_N = mm.reduce_mem_usage(pair6_pred_N)\n",
    "pair6_pred_N.columns = ['id','pair6_predN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_pred_N = pd.merge(pair1_pred_N,pair2_pred_N,on='id',how='inner')\n",
    "monthly_pred_N = pd.merge(monthly_pred_N,pair3_pred_N,on='id',how='inner')\n",
    "monthly_pred_N = pd.merge(monthly_pred_N,pair4_pred_N,on='id',how='inner')\n",
    "monthly_pred_N = pd.merge(monthly_pred_N,pair5_pred_N,on='id',how='inner')\n",
    "monthly_pred_N = pd.merge(monthly_pred_N,pair6_pred_N,on='id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 12.03 MB\n",
      "Memory usage after optimization is: 12.03 MB\n",
      "Decreased by 0.0%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "monthly_pred_N = mm.reduce_mem_usage(monthly_pred_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_pred_N['avg_predN'] = monthly_pred_N.iloc[:,1:7].sum(axis=1)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce Memory Usage Function Reports:\n",
      "Memory usage of dataframe is 75.16 MB\n",
      "Memory usage after optimization is: 30.07 MB\n",
      "Decreased by 60.0%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "monthly_pred_GB = pd.read_csv('Monthly_rolling/monthly_rolling_predGB_combined.csv').drop('Unnamed: 0',axis=1)\n",
    "monthly_pred_GB = mm.reduce_mem_usage(monthly_pred_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_pred_GB['4pairs_avg'] = (monthly_pred_GB['pair2_pred_good']+\n",
    "                                 monthly_pred_GB['pair3_pred_good']+\n",
    "                                 monthly_pred_GB['pair4_pred_good']+\n",
    "                                 monthly_pred_GB['pair5_pred_good'])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>month_mark</th>\n",
       "      <th>pair1_pred_good</th>\n",
       "      <th>pair2_pred_good</th>\n",
       "      <th>pair3_pred_good</th>\n",
       "      <th>pair4_pred_good</th>\n",
       "      <th>pair5_pred_good</th>\n",
       "      <th>pair6_pred_good</th>\n",
       "      <th>avg_pred</th>\n",
       "      <th>bad</th>\n",
       "      <th>4pairs_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56a13c06ee2f2a0db827d0d9450213d3</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.991211</td>\n",
       "      <td>0.984863</td>\n",
       "      <td>0.991211</td>\n",
       "      <td>0.993164</td>\n",
       "      <td>0.991211</td>\n",
       "      <td>0.993164</td>\n",
       "      <td>0.990723</td>\n",
       "      <td>G</td>\n",
       "      <td>0.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c6342448d324d0e99e0d89175a2da335</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.997070</td>\n",
       "      <td>0.997559</td>\n",
       "      <td>0.997070</td>\n",
       "      <td>0.997559</td>\n",
       "      <td>0.997559</td>\n",
       "      <td>0.997070</td>\n",
       "      <td>0.997559</td>\n",
       "      <td>G</td>\n",
       "      <td>0.997559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82e5383d3b794f2555dadb7e37e9108e</td>\n",
       "      <td>2</td>\n",
       "      <td>0.982910</td>\n",
       "      <td>0.986328</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.979492</td>\n",
       "      <td>0.981445</td>\n",
       "      <td>0.982910</td>\n",
       "      <td>G</td>\n",
       "      <td>0.983398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a5e91382fc770d3bf1c3845d113d85a5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.985840</td>\n",
       "      <td>0.988281</td>\n",
       "      <td>0.985352</td>\n",
       "      <td>0.986816</td>\n",
       "      <td>0.981934</td>\n",
       "      <td>0.978516</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>G</td>\n",
       "      <td>0.985840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57e97bcbc4090d25d1c783fcb071b6df</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993164</td>\n",
       "      <td>0.995605</td>\n",
       "      <td>0.995117</td>\n",
       "      <td>0.995117</td>\n",
       "      <td>0.994629</td>\n",
       "      <td>0.993164</td>\n",
       "      <td>0.994629</td>\n",
       "      <td>G</td>\n",
       "      <td>0.994629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985196</th>\n",
       "      <td>edb46b1f6087ba0f55fe280f217c9de0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.945801</td>\n",
       "      <td>0.958496</td>\n",
       "      <td>0.957520</td>\n",
       "      <td>0.967773</td>\n",
       "      <td>0.975098</td>\n",
       "      <td>0.978027</td>\n",
       "      <td>0.963379</td>\n",
       "      <td>G</td>\n",
       "      <td>0.964355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985197</th>\n",
       "      <td>c17c9093f1bce691c6cc292d5af5eeb7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.776855</td>\n",
       "      <td>0.801758</td>\n",
       "      <td>0.833984</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.772461</td>\n",
       "      <td>0.802734</td>\n",
       "      <td>G</td>\n",
       "      <td>0.817383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985198</th>\n",
       "      <td>fe203941c63fd6ca564c62a26ca6f6db</td>\n",
       "      <td>12</td>\n",
       "      <td>0.968262</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.961914</td>\n",
       "      <td>0.967285</td>\n",
       "      <td>0.958984</td>\n",
       "      <td>0.956543</td>\n",
       "      <td>0.963379</td>\n",
       "      <td>G</td>\n",
       "      <td>0.963867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985199</th>\n",
       "      <td>8939c235823b02db54ade065d8f0e128</td>\n",
       "      <td>12</td>\n",
       "      <td>0.777832</td>\n",
       "      <td>0.792969</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.783691</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.791504</td>\n",
       "      <td>G</td>\n",
       "      <td>0.797363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985200</th>\n",
       "      <td>22ca18348111183f8060b772f8eb66a3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.981445</td>\n",
       "      <td>0.981934</td>\n",
       "      <td>0.966309</td>\n",
       "      <td>0.982422</td>\n",
       "      <td>0.981934</td>\n",
       "      <td>0.986816</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>G</td>\n",
       "      <td>0.978027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>985201 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  month_mark  pair1_pred_good  \\\n",
       "0       56a13c06ee2f2a0db827d0d9450213d3        2017         0.991211   \n",
       "1       c6342448d324d0e99e0d89175a2da335        2017         0.997070   \n",
       "2       82e5383d3b794f2555dadb7e37e9108e           2         0.982910   \n",
       "3       a5e91382fc770d3bf1c3845d113d85a5           3         0.985840   \n",
       "4       57e97bcbc4090d25d1c783fcb071b6df           3         0.993164   \n",
       "...                                  ...         ...              ...   \n",
       "985196  edb46b1f6087ba0f55fe280f217c9de0          12         0.945801   \n",
       "985197  c17c9093f1bce691c6cc292d5af5eeb7          12         0.776855   \n",
       "985198  fe203941c63fd6ca564c62a26ca6f6db          12         0.968262   \n",
       "985199  8939c235823b02db54ade065d8f0e128          12         0.777832   \n",
       "985200  22ca18348111183f8060b772f8eb66a3          12         0.981445   \n",
       "\n",
       "        pair2_pred_good  pair3_pred_good  pair4_pred_good  pair5_pred_good  \\\n",
       "0              0.984863         0.991211         0.993164         0.991211   \n",
       "1              0.997559         0.997070         0.997559         0.997559   \n",
       "2              0.986328         0.988281         0.980469         0.979492   \n",
       "3              0.988281         0.985352         0.986816         0.981934   \n",
       "4              0.995605         0.995117         0.995117         0.994629   \n",
       "...                 ...              ...              ...              ...   \n",
       "985196         0.958496         0.957520         0.967773         0.975098   \n",
       "985197         0.801758         0.833984         0.835938         0.796875   \n",
       "985198         0.966797         0.961914         0.967285         0.958984   \n",
       "985199         0.792969         0.769043         0.843750         0.783691   \n",
       "985200         0.981934         0.966309         0.982422         0.981934   \n",
       "\n",
       "        pair6_pred_good  avg_pred bad  4pairs_avg  \n",
       "0              0.993164  0.990723   G    0.990234  \n",
       "1              0.997070  0.997559   G    0.997559  \n",
       "2              0.981445  0.982910   G    0.983398  \n",
       "3              0.978516  0.984375   G    0.985840  \n",
       "4              0.993164  0.994629   G    0.994629  \n",
       "...                 ...       ...  ..         ...  \n",
       "985196         0.978027  0.963379   G    0.964355  \n",
       "985197         0.772461  0.802734   G    0.817383  \n",
       "985198         0.956543  0.963379   G    0.963867  \n",
       "985199         0.781250  0.791504   G    0.797363  \n",
       "985200         0.986816  0.980469   G    0.978027  \n",
       "\n",
       "[985201 rows x 11 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_pred_GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_ks_monthly = {}\n",
    "for i in range(1,13):\n",
    "    \n",
    "    cond = monthly_pred_GB['month_mark'] == i\n",
    "    pred = monthly_pred_GB.loc[cond]['4pairs_avg']\n",
    "    Y_test = monthly_pred_GB['bad'].replace({'G':1,'B':0}).loc[cond]\n",
    "    \n",
    "    auc = roc_auc_score(Y_test,pred)\n",
    "    fpr,tpr,thresholds= roc_curve(Y_test,pred)\n",
    "    ks = max(tpr-fpr)\n",
    "    GB_thres = thresholds[np.argmax(tpr-fpr)]\n",
    "    auc_ks_monthly[str(i)] ={'auc':auc,'ks':ks,'GB_thres':GB_thres}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(auc_ks_monthly).T.to_csv('Monthly_rolling/monthly_performance_4pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
